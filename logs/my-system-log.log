2020-05-28 00:04:12:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.105:51434 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 00:04:12:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.105:51434 in memory (size: 1786.0 B, free: 2004.6 MB)
2020-05-28 00:04:15:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 00:04:15:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 00:04:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c0930c4{/SQL,null,AVAILABLE,@Spark}
2020-05-28 00:04:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7cf66cf9{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 00:04:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@8bde368{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 00:04:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2e9dcdd3{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 00:04:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@68229a6{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 00:04:16:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 00:04:17:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 507.952291 ms
2020-05-28 00:04:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 00:04:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 00:04:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 00:04:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:04:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:04:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 00:04:17:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 00:04:17:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 00:04:17:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.105:51434 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 00:04:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:04:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:04:17:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 00:04:17:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:04:17:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 00:04:17:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:04:17:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 32.151708 ms
2020-05-28 00:05:12:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 00:05:12:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 00:05:13:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 00:05:13:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 00:05:13:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 00:05:13:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 00:05:13:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 00:05:13:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 00:05:14:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 51558.
2020-05-28 00:05:14:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 00:05:14:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 00:05:14:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 00:05:14:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 00:05:14:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-79d46488-9648-4b0e-9af3-063e49b66254
2020-05-28 00:05:14:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 00:05:14:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7424ms
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @7648ms
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@2cdf93f6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 00:05:15:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@46e190ed{/jobs,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@59939293{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d74c81b{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2dbc408c{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78d71df1{/stages,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a9c5b75{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dac121d{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2e7bf7b7{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de81be1{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4519f676{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3596b249{/storage,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@781711b7{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642ee49c{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69909c14{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e224df5{/environment,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5f5827d0{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4337afd{/executors,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3fa7df1{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@52227eb2{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a146b11{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ed5a1b0{/static,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@65bb6275{/,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@72a2312e{/api,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@20580d4e{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d0b05{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 00:05:15:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-28 00:05:15:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 00:05:15:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51559.
2020-05-28 00:05:15:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:51559
2020-05-28 00:05:15:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 00:05:15:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 51559, None)
2020-05-28 00:05:15:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:51559 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 51559, None)
2020-05-28 00:05:15:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 51559, None)
2020-05-28 00:05:15:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 51559, None)
2020-05-28 00:05:16:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@617449dd{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:17:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 00:05:17:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 00:05:17:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:51559 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 00:05:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 00:05:17:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:05:17:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:05:17:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 00:05:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:121
2020-05-28 00:05:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:121) with 1 output partitions
2020-05-28 00:05:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:121)
2020-05-28 00:05:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:05:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:05:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 00:05:18:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:51559 (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:05:18:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 285 ms on localhost (executor driver) (1/1)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:121) finished in 0.319 s
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:121, took 0.574575 s
2020-05-28 00:05:18:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:122
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:122) with 1 output partitions
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:122)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:122), which has no missing parents
2020-05-28 00:05:18:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.105:51559 (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:122) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:05:18:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 916 bytes result sent to driver
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 66 ms on localhost (executor driver) (1/1)
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:122) finished in 0.068 s
2020-05-28 00:05:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:122, took 0.092659 s
2020-05-28 00:05:43:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.105:51559 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 00:05:43:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.105:51559 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 00:05:45:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 00:05:45:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 00:05:45:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@566cc6af{/SQL,null,AVAILABLE,@Spark}
2020-05-28 00:05:45:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7f8f5d37{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:45:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58486deb{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 00:05:45:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@729a9c3d{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 00:05:45:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3a17acd4{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 00:05:47:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 00:05:50:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 729.013874 ms
2020-05-28 00:05:50:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 00:05:50:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 00:05:50:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 00:05:50:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:05:50:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:05:50:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 00:05:50:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 00:05:50:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 00:05:50:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.105:51559 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 00:05:50:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:05:50:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:05:50:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 00:05:50:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:05:50:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 00:05:50:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:05:50:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 16.555197 ms
2020-05-28 00:19:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 00:19:17:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 00:19:18:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 00:19:18:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 00:19:18:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 00:19:18:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 00:19:18:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 00:19:18:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 00:19:19:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 51721.
2020-05-28 00:19:19:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 00:19:19:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 00:19:19:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 00:19:19:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 00:19:19:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-3be7f232-4089-46bf-a5a3-833386eeb61e
2020-05-28 00:19:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 00:19:19:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 00:19:19:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7303ms
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @7575ms
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@50d75d6c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 00:19:20:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@76b224cd{/jobs,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6050462a{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@231baf51{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@34523d46{/stages,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e83c18{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/storage,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/environment,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/executors,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@425d5d46{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4cbd03e7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a639ec5{/static,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@489543a6{/,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6de30571{/api,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@8a62297{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1763992e{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 00:19:20:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-28 00:19:20:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 00:19:20:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51723.
2020-05-28 00:19:20:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:51723
2020-05-28 00:19:20:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 00:19:20:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 51723, None)
2020-05-28 00:19:20:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:51723 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 51723, None)
2020-05-28 00:19:20:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 51723, None)
2020-05-28 00:19:20:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 51723, None)
2020-05-28 00:19:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ab550d5{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:22:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 00:19:22:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 00:19:22:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:51723 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 00:19:22:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 00:19:22:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:19:22:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:19:22:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 00:19:22:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:124
2020-05-28 00:19:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:124) with 1 output partitions
2020-05-28 00:19:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:124)
2020-05-28 00:19:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:19:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:19:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 00:19:22:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 00:19:22:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1709.0 B, free 2004.4 MB)
2020-05-28 00:19:22:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:51723 (size: 1709.0 B, free: 2004.6 MB)
2020-05-28 00:19:22:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:19:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:19:22:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 00:19:22:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:19:22:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 00:19:23:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:19:23:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 298 ms on localhost (executor driver) (1/1)
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:124) finished in 0.331 s
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:124, took 0.584207 s
2020-05-28 00:19:23:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:125
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:125) with 1 output partitions
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:125)
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:125), which has no missing parents
2020-05-28 00:19:23:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 00:19:23:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1789.0 B, free 2004.4 MB)
2020-05-28 00:19:23:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.105:51723 (size: 1789.0 B, free: 2004.6 MB)
2020-05-28 00:19:23:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:125) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:19:23:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 00:19:23:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:19:23:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 60 ms on localhost (executor driver) (1/1)
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:125) finished in 0.062 s
2020-05-28 00:19:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:125, took 0.085795 s
2020-05-28 00:19:24:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.105:51723 in memory (size: 1709.0 B, free: 2004.6 MB)
2020-05-28 00:19:24:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.105:51723 in memory (size: 1789.0 B, free: 2004.6 MB)
2020-05-28 00:19:28:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 00:19:28:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 00:19:28:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6aff97d6{/SQL,null,AVAILABLE,@Spark}
2020-05-28 00:19:28:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@134a8ead{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:28:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5463f035{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 00:19:28:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44fd7ba4{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 00:19:28:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@722531ab{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 00:19:30:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 00:19:33:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 274.77404 ms
2020-05-28 00:19:33:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 00:19:33:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 00:19:33:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 00:19:33:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.105:51723 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 00:19:33:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:19:33:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 00:19:33:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:19:33:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 52.108045 ms
2020-05-28 00:19:33:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 123.493845 ms
2020-05-28 00:19:33:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	... 17 more
2020-05-28 00:19:33:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

2020-05-28 00:19:33:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) failed in 0.468 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

Driver stacktrace:
2020-05-28 00:19:33:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:57, took 0.501752 s
2020-05-28 00:20:08:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 00:20:08:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 00:20:09:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 00:20:09:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 00:20:09:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 00:20:09:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 00:20:09:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 00:20:09:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 00:20:09:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 51756.
2020-05-28 00:20:09:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 00:20:10:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 00:20:10:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 00:20:10:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 00:20:10:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-f3cb785c-8ae4-4f68-8be4-202ae9aa6fe9
2020-05-28 00:20:10:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 00:20:10:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7838ms
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @8002ms
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@4887de2b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 00:20:10:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23cbbd07{/jobs,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3520963d{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1cd43562{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@10b687f2{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@26837057{/stages,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@991cbde{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@456bcb74{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@16a2ed51{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@57ddd45b{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2fb25f4c{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@342e690b{/storage,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78ec89a6{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7237f3c1{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642a16aa{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@294aba23{/environment,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d3b58ca{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@32456db0{/executors,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58a2d9f9{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d02af26{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dd90166{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5ad1904f{/static,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1b01a0d{/,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5261ec9{/api,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ed9f7b1{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@57e388c3{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 00:20:10:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-28 00:20:10:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 00:20:11:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51757.
2020-05-28 00:20:11:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:51757
2020-05-28 00:20:11:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 00:20:11:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 51757, None)
2020-05-28 00:20:11:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:51757 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 51757, None)
2020-05-28 00:20:11:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 51757, None)
2020-05-28 00:20:11:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 51757, None)
2020-05-28 00:20:11:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2e590b{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:12:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 00:20:12:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 00:20:12:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:51757 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 00:20:12:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 00:20:12:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:20:12:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:20:12:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 00:20:12:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:124
2020-05-28 00:20:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:124) with 1 output partitions
2020-05-28 00:20:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:124)
2020-05-28 00:20:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:20:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:20:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 00:20:12:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 00:20:12:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1709.0 B, free 2004.4 MB)
2020-05-28 00:20:12:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:51757 (size: 1709.0 B, free: 2004.6 MB)
2020-05-28 00:20:12:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:20:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:20:12:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:20:13:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 00:20:13:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:20:13:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 289 ms on localhost (executor driver) (1/1)
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:124) finished in 0.324 s
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:124, took 0.602008 s
2020-05-28 00:20:13:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:125
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:125) with 1 output partitions
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:125)
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:125), which has no missing parents
2020-05-28 00:20:13:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 00:20:13:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1789.0 B, free 2004.4 MB)
2020-05-28 00:20:13:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.105:51757 (size: 1789.0 B, free: 2004.6 MB)
2020-05-28 00:20:13:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:125) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:20:13:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 00:20:13:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:20:13:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 73 ms on localhost (executor driver) (1/1)
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:125) finished in 0.076 s
2020-05-28 00:20:13:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:125, took 0.100308 s
2020-05-28 00:20:22:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.105:51757 in memory (size: 1789.0 B, free: 2004.6 MB)
2020-05-28 00:20:22:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.105:51757 in memory (size: 1709.0 B, free: 2004.6 MB)
2020-05-28 00:20:23:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 00:20:23:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 00:20:23:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@47f39279{/SQL,null,AVAILABLE,@Spark}
2020-05-28 00:20:23:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@578198d9{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:23:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7f8f5d37{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 00:20:23:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3ff53704{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 00:20:23:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c7f2fdb{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 00:20:24:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 00:20:25:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 452.211928 ms
2020-05-28 00:20:25:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 00:20:25:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 00:20:25:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 00:20:25:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:20:25:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:20:25:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 00:20:25:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 00:20:25:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 00:20:25:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.105:51757 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 00:20:25:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:20:25:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:20:25:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 00:20:25:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:20:25:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 00:20:25:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:20:25:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 14.381488 ms
2020-05-28 00:21:50:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 00:21:50:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 00:21:50:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 00:21:50:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 00:21:50:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 00:21:50:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 00:21:50:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 00:21:50:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 00:21:51:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 51780.
2020-05-28 00:21:51:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 00:21:51:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 00:21:51:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 00:21:51:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 00:21:51:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-ca164452-ee3e-4898-b123-cddd58bc4637
2020-05-28 00:21:51:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 00:21:52:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @6035ms
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @6245ms
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@300ac922{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 00:21:52:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6dd93a21{/jobs,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@231baf51{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@73877e19{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@34523d46{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e83c18{/stages,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c446b14{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/storage,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/environment,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/executors,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@425d5d46{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4cbd03e7{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a639ec5{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3013909b{/static,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6de30571{/,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3c89bb12{/api,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1763992e{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@659925f4{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 00:21:52:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-28 00:21:53:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 00:21:53:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51781.
2020-05-28 00:21:53:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:51781
2020-05-28 00:21:53:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 00:21:53:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 51781, None)
2020-05-28 00:21:53:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:51781 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 51781, None)
2020-05-28 00:21:53:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 51781, None)
2020-05-28 00:21:53:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 51781, None)
2020-05-28 00:21:53:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58e85c6f{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 00:21:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 00:21:54:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:51781 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 00:21:54:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 00:21:54:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:21:54:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:21:54:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 00:21:54:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:124
2020-05-28 00:21:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:124) with 1 output partitions
2020-05-28 00:21:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:124)
2020-05-28 00:21:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:21:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:21:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 00:21:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 00:21:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-28 00:21:54:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:51781 (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 00:21:54:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:21:55:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 00:21:55:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:21:55:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 880 bytes result sent to driver
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 260 ms on localhost (executor driver) (1/1)
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:124) finished in 0.304 s
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:124, took 0.573563 s
2020-05-28 00:21:55:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:125
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:125) with 1 output partitions
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:125)
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:125), which has no missing parents
2020-05-28 00:21:55:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 00:21:55:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-28 00:21:55:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.105:51781 (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 00:21:55:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:125) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:21:55:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 00:21:55:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:21:55:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 916 bytes result sent to driver
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 72 ms on localhost (executor driver) (1/1)
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:125) finished in 0.074 s
2020-05-28 00:21:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:125, took 0.116423 s
2020-05-28 00:21:56:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.105:51781 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 00:21:56:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.105:51781 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 00:21:57:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 00:21:57:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 00:21:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@38087342{/SQL,null,AVAILABLE,@Spark}
2020-05-28 00:21:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2eb1c615{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7ab63838{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 00:21:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@370c7cc5{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 00:21:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@250967f1{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 00:21:57:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 00:21:58:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 259.566219 ms
2020-05-28 00:21:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 00:21:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 00:21:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 00:21:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:21:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:21:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 00:21:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-28 00:21:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-28 00:21:58:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.105:51781 (size: 6.5 KB, free: 2004.6 MB)
2020-05-28 00:21:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:21:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:21:58:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 00:21:58:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:21:58:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 00:21:58:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:21:58:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 21.973765 ms
2020-05-28 00:21:59:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 82.696284 ms
2020-05-28 00:21:59:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else named_struct(field1, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 0, field1), StringType), true), field2, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 1, field2), StringType), true)) AS myObjects#2
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	... 17 more
2020-05-28 00:21:59:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else named_struct(field1, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 0, field1), StringType), true), field2, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 1, field2), StringType), true)) AS myObjects#2
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

2020-05-28 00:21:59:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-28 00:21:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-28 00:21:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-28 00:21:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) failed in 0.255 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else named_struct(field1, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 0, field1), StringType), true), field2, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 1, field2), StringType), true)) AS myObjects#2
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

Driver stacktrace:
2020-05-28 00:21:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:57, took 0.281859 s
2020-05-28 00:23:41:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 00:23:41:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 00:23:41:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 00:23:41:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 00:23:41:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 00:23:41:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 00:23:41:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 00:23:41:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 00:23:42:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 51804.
2020-05-28 00:23:42:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 00:23:42:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 00:23:42:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 00:23:42:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 00:23:42:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-ed32784a-7941-43e2-ba84-1a284c9cd33c
2020-05-28 00:23:42:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 00:23:42:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @4564ms
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @4753ms
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@60275864{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 00:23:43:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a799159{/jobs,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30b2b76f{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23ee75c5{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30404dba{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c0884e8{/stages,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11841b15{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e83c18{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c446b14{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/storage,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/environment,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/executors,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/static,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4bee18dc{/,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44c5a16f{/api,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6de30571{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3c89bb12{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 00:23:43:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-28 00:23:43:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 00:23:43:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51805.
2020-05-28 00:23:43:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:51805
2020-05-28 00:23:43:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 00:23:43:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 51805, None)
2020-05-28 00:23:43:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:51805 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 51805, None)
2020-05-28 00:23:43:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 51805, None)
2020-05-28 00:23:43:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 51805, None)
2020-05-28 00:23:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6f89292e{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:44:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 00:23:44:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 00:23:44:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:51805 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 00:23:44:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 00:23:44:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:23:44:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:23:44:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 00:23:44:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:125
2020-05-28 00:23:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:125) with 1 output partitions
2020-05-28 00:23:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:125)
2020-05-28 00:23:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:23:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:23:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 00:23:44:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 00:23:44:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-28 00:23:44:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:51805 (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 00:23:44:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:23:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:23:44:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:23:45:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 266 ms on localhost (executor driver) (1/1)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:125) finished in 0.296 s
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:125, took 0.464901 s
2020-05-28 00:23:45:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:126
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:126) with 1 output partitions
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:126)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:126), which has no missing parents
2020-05-28 00:23:45:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.105:51805 (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:126) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:23:45:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 46 ms on localhost (executor driver) (1/1)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:126) finished in 0.048 s
2020-05-28 00:23:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:126, took 0.068913 s
2020-05-28 00:23:45:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.105:51805 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 00:23:45:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.105:51805 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 00:23:46:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 00:23:46:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 00:23:46:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c24f3a2{/SQL,null,AVAILABLE,@Spark}
2020-05-28 00:23:46:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2ec85a25{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:46:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d60059f{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 00:23:46:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@427308f8{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 00:23:46:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44fd7ba4{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 00:23:47:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 00:23:48:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 232.845904 ms
2020-05-28 00:23:48:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 00:23:48:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 00:23:48:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 00:23:48:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.105:51805 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 00:23:48:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:23:48:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 00:23:48:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:23:48:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 14.682854 ms
2020-05-28 00:23:48:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 37.952772 ms
2020-05-28 00:23:48:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	... 17 more
2020-05-28 00:23:48:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

2020-05-28 00:23:48:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) failed in 0.202 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

Driver stacktrace:
2020-05-28 00:23:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:57, took 0.233809 s
2020-05-28 00:23:48:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Invoking stop() from shutdown hook
2020-05-28 00:24:09:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 00:24:10:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 00:24:10:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 00:24:10:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 00:24:10:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 00:24:10:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 00:24:10:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 00:24:10:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 00:24:11:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 51814.
2020-05-28 00:24:11:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 00:24:11:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 00:24:11:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 00:24:11:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 00:24:11:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-375407c7-9751-467b-b6d5-93458673d505
2020-05-28 00:24:12:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 00:24:12:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7910ms
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @8119ms
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@191742e0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 00:24:12:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@46e190ed{/jobs,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@59939293{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d74c81b{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2dbc408c{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78d71df1{/stages,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a9c5b75{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dac121d{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2e7bf7b7{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de81be1{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4519f676{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3596b249{/storage,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@781711b7{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642ee49c{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69909c14{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e224df5{/environment,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5f5827d0{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4337afd{/executors,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3fa7df1{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@52227eb2{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a146b11{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ed5a1b0{/static,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@65bb6275{/,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@72a2312e{/api,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@20580d4e{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d0b05{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 00:24:12:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-28 00:24:13:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 00:24:13:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51815.
2020-05-28 00:24:13:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:51815
2020-05-28 00:24:13:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 00:24:13:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 51815, None)
2020-05-28 00:24:13:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:51815 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 51815, None)
2020-05-28 00:24:13:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 51815, None)
2020-05-28 00:24:13:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 51815, None)
2020-05-28 00:24:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@631c6d11{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:14:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 00:24:14:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 00:24:14:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:51815 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 00:24:14:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 00:24:15:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:24:15:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:24:15:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 00:24:15:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:125
2020-05-28 00:24:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:125) with 1 output partitions
2020-05-28 00:24:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:125)
2020-05-28 00:24:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:24:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:24:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 00:24:15:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 00:24:15:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-28 00:24:15:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:51815 (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 00:24:15:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:24:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:24:15:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 00:24:15:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:24:15:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 00:24:15:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:24:16:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 880 bytes result sent to driver
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 461 ms on localhost (executor driver) (1/1)
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:125) finished in 0.500 s
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:125, took 0.798183 s
2020-05-28 00:24:16:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:126
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:126) with 1 output partitions
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:126)
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:126), which has no missing parents
2020-05-28 00:24:16:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 00:24:16:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-28 00:24:16:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.105:51815 (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 00:24:16:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:126) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:24:16:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 00:24:16:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:24:16:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 87 ms on localhost (executor driver) (1/1)
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:126) finished in 0.090 s
2020-05-28 00:24:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:126, took 0.109556 s
2020-05-28 00:24:21:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.105:51815 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 00:24:21:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.105:51815 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 00:24:21:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 00:24:21:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 00:24:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@32f14274{/SQL,null,AVAILABLE,@Spark}
2020-05-28 00:24:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c86c486{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@348bd063{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 00:24:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@acd115d{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 00:24:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28654aff{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 00:24:21:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 00:24:22:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 252.576151 ms
2020-05-28 00:24:22:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 00:24:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 00:24:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 00:24:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:24:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:24:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 00:24:23:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 00:24:23:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 00:24:23:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.105:51815 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 00:24:23:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:24:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:24:23:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 00:24:23:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:24:23:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 00:24:23:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:24:23:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 14.016251 ms
2020-05-28 00:25:15:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 164.036931 ms
2020-05-28 00:25:15:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	... 17 more
2020-05-28 00:25:15:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

2020-05-28 00:25:15:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-28 00:25:15:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-28 00:25:15:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-28 00:25:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) failed in 52.216 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

Driver stacktrace:
2020-05-28 00:25:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:57, took 52.248459 s
2020-05-28 00:25:40:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 00:25:41:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 00:25:41:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 00:25:41:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 00:25:41:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 00:25:41:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 00:25:41:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 00:25:41:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 00:25:42:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 51832.
2020-05-28 00:25:42:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 00:25:42:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 00:25:42:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 00:25:42:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 00:25:42:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-9fee8d70-4c6a-4e40-b5aa-e468cb83ccac
2020-05-28 00:25:42:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 00:25:43:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @5352ms
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @5658ms
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@14b997a0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 00:25:43:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a799159{/jobs,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30b2b76f{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23ee75c5{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30404dba{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c0884e8{/stages,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11841b15{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e83c18{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c446b14{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/storage,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/environment,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/executors,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/static,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4bee18dc{/,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44c5a16f{/api,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6de30571{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3c89bb12{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 00:25:43:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-28 00:25:44:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 00:25:44:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51833.
2020-05-28 00:25:44:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:51833
2020-05-28 00:25:44:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 00:25:44:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 51833, None)
2020-05-28 00:25:44:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:51833 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 51833, None)
2020-05-28 00:25:44:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 51833, None)
2020-05-28 00:25:44:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 51833, None)
2020-05-28 00:25:44:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6f89292e{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:45:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 00:25:45:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 00:25:45:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:51833 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 00:25:45:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 00:25:45:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:25:45:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:25:45:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 00:25:45:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:125
2020-05-28 00:25:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:125) with 1 output partitions
2020-05-28 00:25:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:125)
2020-05-28 00:25:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:25:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:25:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 00:25:46:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1707.0 B, free 2004.4 MB)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:51833 (size: 1707.0 B, free: 2004.6 MB)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:25:46:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 412 ms on localhost (executor driver) (1/1)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:125) finished in 0.472 s
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:125, took 0.702138 s
2020-05-28 00:25:46:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:126
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:126) with 1 output partitions
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:126)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:126), which has no missing parents
2020-05-28 00:25:46:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1781.0 B, free 2004.4 MB)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.105:51833 (size: 1781.0 B, free: 2004.6 MB)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:126) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:25:46:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 82 ms on localhost (executor driver) (1/1)
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:126) finished in 0.084 s
2020-05-28 00:25:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:126, took 0.109941 s
2020-05-28 00:25:47:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.105:51833 in memory (size: 1781.0 B, free: 2004.6 MB)
2020-05-28 00:25:47:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.105:51833 in memory (size: 1707.0 B, free: 2004.6 MB)
2020-05-28 00:25:48:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 00:25:48:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 00:25:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c24f3a2{/SQL,null,AVAILABLE,@Spark}
2020-05-28 00:25:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2ec85a25{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d60059f{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 00:25:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@427308f8{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 00:25:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44fd7ba4{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 00:25:49:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 00:25:50:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 381.287087 ms
2020-05-28 00:25:51:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 00:25:51:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 00:25:51:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 00:25:51:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.105:51833 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 00:25:51:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:25:51:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 00:25:51:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:25:51:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 16.399596 ms
2020-05-28 00:25:51:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 41.93154 ms
2020-05-28 00:25:51:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2). 1170 bytes result sent to driver
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2) in 175 ms on localhost (executor driver) (1/1)
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) finished in 0.178 s
2020-05-28 00:25:51:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 finished: show at XmlTransformerTest.scala:57, took 0.211561 s
2020-05-28 00:26:05:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 00:26:05:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 00:26:05:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 00:26:05:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 00:26:05:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 00:26:05:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 00:26:05:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 00:26:05:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 00:26:06:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 51839.
2020-05-28 00:26:06:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 00:26:06:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 00:26:06:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 00:26:06:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 00:26:06:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-ff98b7cf-dd67-4014-9b7b-47f5c1f23341
2020-05-28 00:26:06:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 00:26:06:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 00:26:06:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @6421ms
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @6586ms
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@516ada06{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 00:26:07:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@46e190ed{/jobs,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@59939293{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d74c81b{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2dbc408c{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78d71df1{/stages,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a9c5b75{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dac121d{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2e7bf7b7{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de81be1{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4519f676{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3596b249{/storage,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@781711b7{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642ee49c{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69909c14{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e224df5{/environment,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5f5827d0{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4337afd{/executors,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3fa7df1{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@52227eb2{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a146b11{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ed5a1b0{/static,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@65bb6275{/,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@72a2312e{/api,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@20580d4e{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d0b05{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 00:26:07:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-28 00:26:07:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 00:26:07:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51840.
2020-05-28 00:26:07:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:51840
2020-05-28 00:26:07:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 00:26:07:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 51840, None)
2020-05-28 00:26:07:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:51840 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 51840, None)
2020-05-28 00:26:07:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 51840, None)
2020-05-28 00:26:07:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 51840, None)
2020-05-28 00:26:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5e0c4f21{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:08:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 00:26:08:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 00:26:08:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:51840 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 00:26:08:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 00:26:09:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:26:09:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 00:26:09:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 00:26:09:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:125
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:125) with 1 output partitions
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:125)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 00:26:09:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1709.0 B, free 2004.4 MB)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:51840 (size: 1709.0 B, free: 2004.6 MB)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:26:09:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 308 ms on localhost (executor driver) (1/1)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:125) finished in 0.353 s
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:125, took 0.559997 s
2020-05-28 00:26:09:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:126
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:126) with 1 output partitions
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:126)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:126), which has no missing parents
2020-05-28 00:26:09:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1788.0 B, free 2004.4 MB)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.105:51840 (size: 1788.0 B, free: 2004.6 MB)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:126) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:26:09:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 66 ms on localhost (executor driver) (1/1)
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:126) finished in 0.068 s
2020-05-28 00:26:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:126, took 0.086527 s
2020-05-28 00:26:17:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.105:51840 in memory (size: 1709.0 B, free: 2004.6 MB)
2020-05-28 00:26:17:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.105:51840 in memory (size: 1788.0 B, free: 2004.6 MB)
2020-05-28 00:26:19:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 00:26:19:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 00:26:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@782dc5{/SQL,null,AVAILABLE,@Spark}
2020-05-28 00:26:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d20195b{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1e1598e5{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 00:26:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@34e4fb39{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 00:26:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6705b763{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 00:26:20:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 00:26:21:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 230.167669 ms
2020-05-28 00:26:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 00:26:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 00:26:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 00:26:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 00:26:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 00:26:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 00:26:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 00:26:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 00:26:21:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.105:51840 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 00:26:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 00:26:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 00:26:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 00:26:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 00:26:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 00:26:21:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 00:26:21:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 15.27172 ms
2020-05-28 23:51:50:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 23:51:50:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 23:51:51:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 23:51:51:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 23:51:51:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 23:51:51:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 23:51:51:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 23:51:51:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 23:51:51:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 54910.
2020-05-28 23:51:51:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 23:51:51:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 23:51:51:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 23:51:51:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 23:51:51:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-432110c2-8e8b-4d24-8864-b828492f301d
2020-05-28 23:51:51:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 23:51:52:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @4854ms
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @5090ms
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@24d5a00{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 23:51:52:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a799159{/jobs,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30b2b76f{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23ee75c5{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30404dba{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c0884e8{/stages,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11841b15{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e83c18{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c446b14{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/storage,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/environment,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/executors,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/static,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4bee18dc{/,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44c5a16f{/api,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6de30571{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3c89bb12{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 23:51:52:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.106:4040
2020-05-28 23:51:53:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 23:51:53:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54911.
2020-05-28 23:51:53:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.106:54911
2020-05-28 23:51:53:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 23:51:53:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.106, 54911, None)
2020-05-28 23:51:53:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.106:54911 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.106, 54911, None)
2020-05-28 23:51:53:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.106, 54911, None)
2020-05-28 23:51:53:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.106, 54911, None)
2020-05-28 23:51:53:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6f89292e{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.106:54911 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 23:51:54:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 23:51:54:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 23:51:54:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 23:51:54:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:126
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:126) with 1 output partitions
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:126)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 23:51:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.106:54911 (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:51:54:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 880 bytes result sent to driver
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 268 ms on localhost (executor driver) (1/1)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:126) finished in 0.302 s
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:126, took 0.472109 s
2020-05-28 23:51:54:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:127
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:127) with 1 output partitions
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:127)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:127), which has no missing parents
2020-05-28 23:51:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.106:54911 (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:127) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 23:51:54:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 23:51:54:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:51:55:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-28 23:51:55:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 51 ms on localhost (executor driver) (1/1)
2020-05-28 23:51:55:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 23:51:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:127) finished in 0.053 s
2020-05-28 23:51:55:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:127, took 0.070252 s
2020-05-28 23:51:56:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.106:54911 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 23:51:56:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.106:54911 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 23:51:56:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 23:51:56:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 23:51:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c24f3a2{/SQL,null,AVAILABLE,@Spark}
2020-05-28 23:51:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2ec85a25{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d60059f{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 23:51:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@427308f8{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 23:51:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44fd7ba4{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 23:51:57:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 23:51:58:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 274.933623 ms
2020-05-28 23:51:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 23:51:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 23:51:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 23:51:58:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.106:54911 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 23:51:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:51:58:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 23:51:58:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:51:58:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 15.145203 ms
2020-05-28 23:51:58:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
scala.MatchError: [Ljava.lang.String;@4e28ea6a (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:86) ~[classes/:?]
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:86) ~[classes/:?]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.immutable.List.foreach(List.scala:392) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.immutable.List.map(List.scala:296) ~[scala-library-2.11.11.jar:?]
	at core.XmlTransformer$.temp(XmlTransformer.scala:86) ~[classes/:?]
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:136) ~[classes/:?]
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:133) ~[classes/:?]
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2020-05-28 23:51:58:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@4e28ea6a (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:86)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:86)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at core.XmlTransformer$.temp(XmlTransformer.scala:86)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:136)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:133)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2020-05-28 23:51:58:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) failed in 0.136 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@4e28ea6a (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:86)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:86)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at core.XmlTransformer$.temp(XmlTransformer.scala:86)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:136)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:133)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
2020-05-28 23:51:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:57, took 0.166301 s
2020-05-28 23:54:39:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 23:54:39:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 23:54:40:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 23:54:40:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 23:54:40:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 23:54:40:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 23:54:40:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 23:54:40:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 23:54:41:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 54952.
2020-05-28 23:54:41:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 23:54:41:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 23:54:41:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 23:54:41:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 23:54:41:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-2618f9a4-e19b-4440-b854-dba0ff05b254
2020-05-28 23:54:41:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 23:54:41:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7135ms
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @7407ms
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@703ed7a9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 23:54:42:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6ff37443{/jobs,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23ee75c5{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@340b7ef6{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c0884e8{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11841b15{/stages,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@34523d46{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c446b14{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/storage,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/environment,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/executors,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@425d5d46{/static,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44c5a16f{/,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a6ebe1e{/api,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3c89bb12{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3df978b9{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 23:54:42:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.106:4040
2020-05-28 23:54:43:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 23:54:43:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54953.
2020-05-28 23:54:43:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.106:54953
2020-05-28 23:54:43:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 23:54:43:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.106, 54953, None)
2020-05-28 23:54:43:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.106:54953 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.106, 54953, None)
2020-05-28 23:54:43:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.106, 54953, None)
2020-05-28 23:54:43:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.106, 54953, None)
2020-05-28 23:54:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de77232{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:44:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 23:54:44:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 23:54:44:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.106:54953 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 23:54:44:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 23:54:45:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 23:54:45:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 23:54:45:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 23:54:45:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:127
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:127) with 1 output partitions
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:127)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 23:54:45:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.106:54953 (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:54:45:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 880 bytes result sent to driver
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 366 ms on localhost (executor driver) (1/1)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:127) finished in 0.405 s
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:127, took 0.640376 s
2020-05-28 23:54:45:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:128
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:128) with 1 output partitions
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:128)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128), which has no missing parents
2020-05-28 23:54:45:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.106:54953 (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 23:54:45:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 23:54:45:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:54:46:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-28 23:54:46:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 99 ms on localhost (executor driver) (1/1)
2020-05-28 23:54:46:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 23:54:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:128) finished in 0.098 s
2020-05-28 23:54:46:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:128, took 0.123021 s
2020-05-28 23:54:47:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.106:54953 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-28 23:54:47:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.106:54953 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 23:54:49:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 23:54:49:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 23:54:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2ec85a25{/SQL,null,AVAILABLE,@Spark}
2020-05-28 23:54:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@27c53c32{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@427308f8{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 23:54:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4975dda1{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 23:54:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69d103f0{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 23:54:50:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 23:54:52:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 712.055107 ms
2020-05-28 23:54:52:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 23:54:52:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 23:54:52:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 23:54:52:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.106:54953 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 23:54:52:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:54:52:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 23:54:52:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:54:52:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 15.084858 ms
2020-05-28 23:54:52:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 70.55152 ms
2020-05-28 23:54:52:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: [C is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.lang.RuntimeException: [C is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	... 17 more
2020-05-28 23:54:52:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: [C is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: [C is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

2020-05-28 23:54:52:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) failed in 0.316 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: [C is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: [C is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

Driver stacktrace:
2020-05-28 23:54:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:57, took 0.344622 s
2020-05-28 23:56:13:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 23:56:13:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 23:56:14:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 23:56:14:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 23:56:14:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 23:56:14:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 23:56:14:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 23:56:14:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 23:56:15:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 54976.
2020-05-28 23:56:15:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 23:56:15:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 23:56:15:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 23:56:15:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 23:56:15:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-bef23b9b-2b70-49b0-ac6c-84da1fce24e4
2020-05-28 23:56:15:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 23:56:15:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7807ms
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @8007ms
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@695a33fb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 23:56:15:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@46e190ed{/jobs,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@59939293{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d74c81b{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2dbc408c{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78d71df1{/stages,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a9c5b75{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dac121d{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2e7bf7b7{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de81be1{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4519f676{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3596b249{/storage,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@781711b7{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642ee49c{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69909c14{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e224df5{/environment,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5f5827d0{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4337afd{/executors,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3fa7df1{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@52227eb2{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a146b11{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ed5a1b0{/static,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@65bb6275{/,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@72a2312e{/api,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@20580d4e{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d0b05{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 23:56:15:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.106:4040
2020-05-28 23:56:16:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 23:56:16:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54977.
2020-05-28 23:56:16:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.106:54977
2020-05-28 23:56:16:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 23:56:16:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.106, 54977, None)
2020-05-28 23:56:16:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.106:54977 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.106, 54977, None)
2020-05-28 23:56:16:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.106, 54977, None)
2020-05-28 23:56:16:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.106, 54977, None)
2020-05-28 23:56:16:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@617449dd{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 23:56:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 23:56:19:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.106:54977 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 23:56:19:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 23:56:20:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 23:56:20:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 23:56:20:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 23:56:20:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:127
2020-05-28 23:56:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:127) with 1 output partitions
2020-05-28 23:56:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:127)
2020-05-28 23:56:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:56:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:56:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 23:56:20:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 23:56:20:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1709.0 B, free 2004.4 MB)
2020-05-28 23:56:20:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.106:54977 (size: 1709.0 B, free: 2004.6 MB)
2020-05-28 23:56:20:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:56:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:56:20:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 23:56:20:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:56:20:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 23:56:21:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:56:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 651 ms on localhost (executor driver) (1/1)
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:127) finished in 0.698 s
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:127, took 1.220487 s
2020-05-28 23:56:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:128
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:128) with 1 output partitions
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:128)
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128), which has no missing parents
2020-05-28 23:56:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 23:56:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1786.0 B, free 2004.4 MB)
2020-05-28 23:56:21:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.106:54977 (size: 1786.0 B, free: 2004.6 MB)
2020-05-28 23:56:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:56:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 23:56:21:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:56:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 64 ms on localhost (executor driver) (1/1)
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:128) finished in 0.065 s
2020-05-28 23:56:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:128, took 0.084221 s
2020-05-28 23:56:29:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.106:54977 in memory (size: 1786.0 B, free: 2004.6 MB)
2020-05-28 23:56:29:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.106:54977 in memory (size: 1709.0 B, free: 2004.6 MB)
2020-05-28 23:56:32:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 23:56:32:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 23:56:32:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5b0835cb{/SQL,null,AVAILABLE,@Spark}
2020-05-28 23:56:32:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@62615be{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:32:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6493f780{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 23:56:32:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@35f22eef{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 23:56:32:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@67a3394c{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 23:56:33:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 23:56:35:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 476.229213 ms
2020-05-28 23:56:35:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 23:56:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 23:56:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 23:56:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:56:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:56:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 23:56:35:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 23:56:35:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 23:56:35:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.106:54977 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 23:56:35:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:56:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:56:35:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 23:56:35:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:56:35:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 23:56:35:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:56:35:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 19.012922 ms
2020-05-28 23:57:55:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-28 23:57:55:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-28 23:57:56:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-28 23:57:56:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-28 23:57:56:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-28 23:57:56:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-28 23:57:56:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-28 23:57:56:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-28 23:57:56:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 55022.
2020-05-28 23:57:56:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-28 23:57:56:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-28 23:57:56:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-28 23:57:56:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-28 23:57:56:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-b8ef22e9-4035-4950-a2b7-218d416e2e94
2020-05-28 23:57:57:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-28 23:57:57:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @4387ms
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @4540ms
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@6555d093{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-28 23:57:57:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@726a17c4{/jobs,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2eb79cbe{/jobs/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@43826ec{/jobs/job,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7578e06a{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@56da52a7{/stages,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11a7ba62{/stages/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30404dba{/stages/stage,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11841b15{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/stages/pool,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@34523d46{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e83c18{/storage,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/storage/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c446b14{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/environment,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/environment/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/executors,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/executors/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/static,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@982bb90{/,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7bef452c{/api,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44c5a16f{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a6ebe1e{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-28 23:57:57:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.106:4040
2020-05-28 23:57:57:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-28 23:57:57:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55023.
2020-05-28 23:57:57:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.106:55023
2020-05-28 23:57:57:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-28 23:57:57:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.106, 55023, None)
2020-05-28 23:57:57:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.106:55023 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.106, 55023, None)
2020-05-28 23:57:57:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.106, 55023, None)
2020-05-28 23:57:57:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.106, 55023, None)
2020-05-28 23:57:58:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6f89292e{/metrics/json,null,AVAILABLE,@Spark}
2020-05-28 23:57:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-28 23:57:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-28 23:57:58:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.106:55023 (size: 20.5 KB, free: 2004.6 MB)
2020-05-28 23:57:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-28 23:57:58:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 23:57:58:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-28 23:57:58:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-28 23:57:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:127
2020-05-28 23:57:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:127) with 1 output partitions
2020-05-28 23:57:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:127)
2020-05-28 23:57:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:57:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:57:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-28 23:57:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-28 23:57:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1709.0 B, free 2004.4 MB)
2020-05-28 23:57:58:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.106:55023 (size: 1709.0 B, free: 2004.6 MB)
2020-05-28 23:57:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:57:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:57:58:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:57:59:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 236 ms on localhost (executor driver) (1/1)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:127) finished in 0.262 s
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:127, took 0.407823 s
2020-05-28 23:57:59:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:128
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:128) with 1 output partitions
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:128)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128), which has no missing parents
2020-05-28 23:57:59:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.106:55023 (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:57:59:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 48 ms on localhost (executor driver) (1/1)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:128) finished in 0.050 s
2020-05-28 23:57:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:128, took 0.068974 s
2020-05-28 23:57:59:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.106:55023 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-28 23:57:59:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.106:55023 in memory (size: 1709.0 B, free: 2004.6 MB)
2020-05-28 23:58:00:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-28 23:58:00:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-28 23:58:00:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c24f3a2{/SQL,null,AVAILABLE,@Spark}
2020-05-28 23:58:00:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2ec85a25{/SQL/json,null,AVAILABLE,@Spark}
2020-05-28 23:58:00:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d60059f{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-28 23:58:00:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@427308f8{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-28 23:58:00:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44fd7ba4{/static/sql,null,AVAILABLE,@Spark}
2020-05-28 23:58:01:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-28 23:58:01:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 186.820306 ms
2020-05-28 23:58:01:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-28 23:58:01:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-28 23:58:01:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-28 23:58:01:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-28 23:58:01:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-28 23:58:01:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-28 23:58:01:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-28 23:58:01:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-28 23:58:01:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.106:55023 (size: 5.7 KB, free: 2004.6 MB)
2020-05-28 23:58:01:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-28 23:58:01:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-28 23:58:01:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-28 23:58:01:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-28 23:58:01:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-28 23:58:01:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-28 23:58:01:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 10.416774 ms
2020-05-28 23:58:02:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 35.979749 ms
2020-05-28 23:58:02:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	... 17 more
2020-05-28 23:58:02:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

2020-05-28 23:58:02:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-28 23:58:02:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-28 23:58:02:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-28 23:58:02:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) failed in 0.172 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: scala.collection.immutable.$colon$colon is not a valid external type for schema of string
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfFalseExpr$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

Driver stacktrace:
2020-05-28 23:58:02:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:57, took 0.196197 s
2020-05-29 00:22:05:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 00:22:06:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 00:22:06:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 00:22:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 00:22:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 00:22:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 00:22:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 00:22:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 00:22:07:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 55281.
2020-05-29 00:22:07:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 00:22:07:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 00:22:07:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 00:22:07:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 00:22:07:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-c17e78df-c711-4ce2-afe7-04bfc60d45cc
2020-05-29 00:22:07:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 00:22:08:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7970ms
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @8134ms
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@42743232{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 00:22:08:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23cbbd07{/jobs,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3520963d{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1cd43562{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@10b687f2{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@26837057{/stages,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@991cbde{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@456bcb74{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@16a2ed51{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@57ddd45b{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2fb25f4c{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@342e690b{/storage,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78ec89a6{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7237f3c1{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642a16aa{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@294aba23{/environment,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d3b58ca{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@32456db0{/executors,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58a2d9f9{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d02af26{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dd90166{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5ad1904f{/static,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1b01a0d{/,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5261ec9{/api,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ed9f7b1{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@57e388c3{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 00:22:08:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.106:4040
2020-05-29 00:22:08:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 00:22:08:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55282.
2020-05-29 00:22:08:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.106:55282
2020-05-29 00:22:08:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 00:22:08:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.106, 55282, None)
2020-05-29 00:22:08:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.106:55282 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.106, 55282, None)
2020-05-29 00:22:08:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.106, 55282, None)
2020-05-29 00:22:08:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.106, 55282, None)
2020-05-29 00:22:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5e0c4f21{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:10:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 00:22:10:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 00:22:10:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.106:55282 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 00:22:10:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 00:22:10:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 00:22:10:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 00:22:10:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 00:22:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:127
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:127) with 1 output partitions
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:127)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 00:22:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.106:55282 (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:22:11:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 304 ms on localhost (executor driver) (1/1)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:127) finished in 0.340 s
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:127, took 0.721964 s
2020-05-29 00:22:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:128
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:128) with 1 output partitions
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:128)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128), which has no missing parents
2020-05-29 00:22:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.106:55282 (size: 1793.0 B, free: 2004.6 MB)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 00:22:11:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 00:22:11:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:22:12:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-29 00:22:12:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 69 ms on localhost (executor driver) (1/1)
2020-05-29 00:22:12:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 00:22:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:128) finished in 0.073 s
2020-05-29 00:22:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:128, took 0.107422 s
2020-05-29 00:22:41:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.106:55282 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-29 00:22:41:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.106:55282 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 00:22:42:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 00:22:42:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 00:22:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42734b71{/SQL,null,AVAILABLE,@Spark}
2020-05-29 00:22:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@70e54ec3{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51aa2a58{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 00:22:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@56554365{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 00:22:42:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@50930bff{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 00:22:43:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 00:22:44:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 250.355328 ms
2020-05-29 00:22:44:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-29 00:22:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-29 00:22:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-29 00:22:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:22:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:22:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-29 00:22:44:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-29 00:22:44:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-29 00:22:44:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.106:55282 (size: 5.7 KB, free: 2004.6 MB)
2020-05-29 00:22:44:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:22:44:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:22:44:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 00:22:44:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:22:44:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 00:22:44:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:22:44:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 12.644628 ms
2020-05-29 00:36:14:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 00:36:14:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 00:36:15:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 00:36:15:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 00:36:15:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 00:36:15:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 00:36:15:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 00:36:15:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 00:36:16:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 55479.
2020-05-29 00:36:16:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 00:36:16:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 00:36:16:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 00:36:16:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 00:36:16:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-43859199-ab87-4e67-b7e5-bceea822c28d
2020-05-29 00:36:16:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 00:36:16:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 00:36:16:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @6074ms
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @6284ms
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@63e5e5b4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 00:36:17:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@77ee25f1{/jobs,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c0884e8{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11841b15{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5bfc257{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7cedfa63{/stages,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@783efb48{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e8e8621{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7e46d648{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2b0b4d53{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@38548b19{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@303a5119{/storage,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1bcb79c2{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6fca2a8f{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@71391b3f{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@12db3386{/environment,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@79a1728c{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41f35f7c{/executors,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3005db4a{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@198ef2ce{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@52fc5eb1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@14151bc5{/static,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6272c96f{/,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5ee34b1b{/api,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@61a91912{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5c92166b{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 00:36:17:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.106:4040
2020-05-29 00:36:17:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 00:36:17:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55480.
2020-05-29 00:36:17:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.106:55480
2020-05-29 00:36:17:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 00:36:17:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.106, 55480, None)
2020-05-29 00:36:17:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.106:55480 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.106, 55480, None)
2020-05-29 00:36:17:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.106, 55480, None)
2020-05-29 00:36:17:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.106, 55480, None)
2020-05-29 00:36:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3cd206b5{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:18:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.106:55480 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 00:36:18:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 00:36:18:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 00:36:18:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 00:36:18:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:127
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:127) with 1 output partitions
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:127)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 00:36:18:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1709.0 B, free 2004.4 MB)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.106:55480 (size: 1709.0 B, free: 2004.6 MB)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:36:18:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 263 ms on localhost (executor driver) (1/1)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:127) finished in 0.293 s
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:127, took 0.443494 s
2020-05-29 00:36:18:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:128
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:128) with 1 output partitions
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:128)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128), which has no missing parents
2020-05-29 00:36:18:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.106:55480 (size: 1793.0 B, free: 2004.6 MB)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:36:18:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 916 bytes result sent to driver
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 55 ms on localhost (executor driver) (1/1)
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:128) finished in 0.056 s
2020-05-29 00:36:18:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:128, took 0.071366 s
2020-05-29 00:36:19:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.106:55480 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-29 00:36:19:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.106:55480 in memory (size: 1709.0 B, free: 2004.6 MB)
2020-05-29 00:36:20:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 00:36:20:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 00:36:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d60059f{/SQL,null,AVAILABLE,@Spark}
2020-05-29 00:36:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@427308f8{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@230232b0{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 00:36:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@22f8adc2{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 00:36:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@8f57e4c{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 00:36:20:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 00:36:21:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 211.415159 ms
2020-05-29 00:36:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-29 00:36:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.4 MB)
2020-05-29 00:36:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KB, free 2004.4 MB)
2020-05-29 00:36:21:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.106:55480 (size: 5.7 KB, free: 2004.6 MB)
2020-05-29 00:36:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:36:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 00:36:21:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:36:21:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 14.059392 ms
2020-05-29 00:36:21:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 42.256376 ms
2020-05-29 00:36:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2). 1167 bytes result sent to driver
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2) in 134 ms on localhost (executor driver) (1/1)
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) finished in 0.136 s
2020-05-29 00:36:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 finished: show at XmlTransformerTest.scala:57, took 0.171052 s
2020-05-29 00:37:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 00:37:18:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 00:37:18:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 00:37:18:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 00:37:18:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 00:37:18:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 00:37:18:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 00:37:18:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 00:37:19:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 55490.
2020-05-29 00:37:19:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 00:37:19:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 00:37:19:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 00:37:19:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 00:37:19:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-2aad621c-20d7-4073-afbc-13bfe023b4ee
2020-05-29 00:37:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 00:37:19:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @4062ms
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @4218ms
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@1876f80a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 00:37:19:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a799159{/jobs,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30b2b76f{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23ee75c5{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30404dba{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c0884e8{/stages,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11841b15{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e83c18{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c446b14{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/storage,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/environment,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/executors,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/static,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4bee18dc{/,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44c5a16f{/api,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6de30571{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3c89bb12{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 00:37:19:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.106:4040
2020-05-29 00:37:19:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 00:37:19:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55491.
2020-05-29 00:37:19:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.106:55491
2020-05-29 00:37:19:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 00:37:19:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.106, 55491, None)
2020-05-29 00:37:19:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.106:55491 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.106, 55491, None)
2020-05-29 00:37:19:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.106, 55491, None)
2020-05-29 00:37:19:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.106, 55491, None)
2020-05-29 00:37:20:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@26a262d6{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:20:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 00:37:20:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 00:37:20:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.106:55491 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 00:37:20:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 00:37:21:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 00:37:21:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 00:37:21:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 00:37:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:127
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:127) with 1 output partitions
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:127)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 00:37:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.106:55491 (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:37:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 880 bytes result sent to driver
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 257 ms on localhost (executor driver) (1/1)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:127) finished in 0.295 s
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:127, took 0.487357 s
2020-05-29 00:37:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:128
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:128) with 1 output partitions
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:128)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128), which has no missing parents
2020-05-29 00:37:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.106:55491 (size: 1793.0 B, free: 2004.6 MB)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:37:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 916 bytes result sent to driver
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 44 ms on localhost (executor driver) (1/1)
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:128) finished in 0.047 s
2020-05-29 00:37:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:128, took 0.062686 s
2020-05-29 00:37:22:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.106:55491 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 00:37:22:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.106:55491 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-29 00:37:23:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 00:37:23:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 00:37:23:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e17442f{/SQL,null,AVAILABLE,@Spark}
2020-05-29 00:37:23:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@62e73ab6{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:23:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7ddeb27f{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 00:37:23:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@681de87f{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 00:37:23:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7ab63838{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 00:37:23:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 00:37:24:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 191.785571 ms
2020-05-29 00:37:24:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-29 00:37:24:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-29 00:37:24:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-29 00:37:24:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.106:55491 (size: 6.5 KB, free: 2004.6 MB)
2020-05-29 00:37:24:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:37:24:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 00:37:24:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:37:24:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 17.217497 ms
2020-05-29 00:37:24:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 55.696405 ms
2020-05-29 00:37:24:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.RuntimeException: Error while encoding: java.lang.ArrayIndexOutOfBoundsException: 2
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else named_struct(field1, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 0, field1), StringType), true), field2, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 1, field2), StringType), true)) AS myObjects#2
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.spark.sql.catalyst.expressions.GenericRow.get(rows.scala:173) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.Row$class.isNullAt(Row.scala:191) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.catalyst.expressions.GenericRow.isNullAt(rows.scala:165) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfCondExpr4$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287) ~[spark-catalyst_2.11-2.2.1.jar:2.2.1]
	... 17 more
2020-05-29 00:37:24:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.ArrayIndexOutOfBoundsException: 2
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else named_struct(field1, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 0, field1), StringType), true), field2, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 1, field2), StringType), true)) AS myObjects#2
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.spark.sql.catalyst.expressions.GenericRow.get(rows.scala:173)
	at org.apache.spark.sql.Row$class.isNullAt(Row.scala:191)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.isNullAt(rows.scala:165)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfCondExpr4$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

2020-05-29 00:37:24:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) failed in 0.200 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: Error while encoding: java.lang.ArrayIndexOutOfBoundsException: 2
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, pk), StringType), true) AS pk#0
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, name), StringType), true) AS name#1
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else named_struct(field1, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 0, field1), StringType), true), field2, if (validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, myObjects), StructField(field1,StringType,true), StructField(field2,StringType,true)), 1, field2), StringType), true)) AS myObjects#2
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:582)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.spark.sql.catalyst.expressions.GenericRow.get(rows.scala:173)
	at org.apache.spark.sql.Row$class.isNullAt(Row.scala:191)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.isNullAt(rows.scala:165)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.evalIfCondExpr4$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)
	... 17 more

Driver stacktrace:
2020-05-29 00:37:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:57, took 0.223306 s
2020-05-29 00:37:24:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Invoking stop() from shutdown hook
2020-05-29 00:38:04:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 00:38:05:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 00:38:05:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 00:38:05:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 00:38:05:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 00:38:05:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 00:38:05:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 00:38:05:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 00:38:05:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 55504.
2020-05-29 00:38:05:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 00:38:05:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 00:38:05:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 00:38:05:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 00:38:06:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-3caf9fcd-7c49-4e71-9e60-b3d73d1731de
2020-05-29 00:38:06:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 00:38:06:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @5880ms
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @6016ms
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@43064e24{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 00:38:06:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@485caa8f{/jobs,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@10b687f2{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@26837057{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@456bcb74{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@dd4aec3{/stages,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@568750b7{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@16a2ed51{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@342e690b{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78ec89a6{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7237f3c1{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642a16aa{/storage,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@294aba23{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d3b58ca{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@32456db0{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58a2d9f9{/environment,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d02af26{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dd90166{/executors,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5ad1904f{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@f438904{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@18d003cd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@22ad1bae{/static,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@47c7a9e5{/,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ed9f7b1{/api,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4b432e53{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@62f11ebb{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 00:38:06:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.106:4040
2020-05-29 00:38:06:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 00:38:06:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55505.
2020-05-29 00:38:06:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.106:55505
2020-05-29 00:38:06:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 00:38:06:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.106, 55505, None)
2020-05-29 00:38:06:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.106:55505 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.106, 55505, None)
2020-05-29 00:38:06:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.106, 55505, None)
2020-05-29 00:38:06:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.106, 55505, None)
2020-05-29 00:38:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1df5c7e3{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:08:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 00:38:08:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 00:38:08:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.106:55505 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 00:38:08:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 00:38:08:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 00:38:08:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 00:38:08:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 00:38:08:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:127
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:127) with 1 output partitions
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:127)
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 00:38:08:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 00:38:08:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1709.0 B, free 2004.4 MB)
2020-05-29 00:38:08:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.106:55505 (size: 1709.0 B, free: 2004.6 MB)
2020-05-29 00:38:08:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:38:08:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 00:38:08:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:38:08:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 794 bytes result sent to driver
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 271 ms on localhost (executor driver) (1/1)
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:127) finished in 0.304 s
2020-05-29 00:38:08:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:127, took 0.504772 s
2020-05-29 00:38:09:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:128
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:128) with 1 output partitions
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:128)
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128), which has no missing parents
2020-05-29 00:38:09:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 00:38:09:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-29 00:38:09:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.106:55505 (size: 1793.0 B, free: 2004.6 MB)
2020-05-29 00:38:09:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:38:09:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 00:38:09:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:38:09:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 916 bytes result sent to driver
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 59 ms on localhost (executor driver) (1/1)
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:128) finished in 0.060 s
2020-05-29 00:38:09:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:128, took 0.076409 s
2020-05-29 00:38:27:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 00:38:27:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 00:38:28:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 00:38:28:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 00:38:28:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 00:38:28:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 00:38:28:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 00:38:28:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 00:38:28:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 55511.
2020-05-29 00:38:28:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 00:38:28:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 00:38:28:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 00:38:28:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 00:38:28:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-89862476-70f2-430f-a447-8d0d1b1d2b0e
2020-05-29 00:38:28:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 00:38:29:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @4218ms
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @4344ms
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@138eb6dc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 00:38:29:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@76b224cd{/jobs,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6050462a{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@231baf51{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@34523d46{/stages,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e83c18{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/storage,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/environment,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/executors,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@425d5d46{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4cbd03e7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a639ec5{/static,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@489543a6{/,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6de30571{/api,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@8a62297{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1763992e{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 00:38:29:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.106:4040
2020-05-29 00:38:29:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 00:38:29:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55512.
2020-05-29 00:38:29:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.106:55512
2020-05-29 00:38:29:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 00:38:29:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.106, 55512, None)
2020-05-29 00:38:29:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.106:55512 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.106, 55512, None)
2020-05-29 00:38:29:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.106, 55512, None)
2020-05-29 00:38:29:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.106, 55512, None)
2020-05-29 00:38:29:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ab550d5{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:30:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 00:38:30:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 00:38:30:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.106:55512 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 00:38:30:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 00:38:30:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 00:38:30:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 00:38:30:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 00:38:30:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:127
2020-05-29 00:38:30:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:127) with 1 output partitions
2020-05-29 00:38:30:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:127)
2020-05-29 00:38:30:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:38:30:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:38:30:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 00:38:30:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 00:38:30:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-29 00:38:30:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.106:55512 (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 00:38:30:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:38:30:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:38:30:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:38:31:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 00:38:31:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:38:31:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 880 bytes result sent to driver
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 388 ms on localhost (executor driver) (1/1)
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:127) finished in 0.456 s
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:127, took 0.669964 s
2020-05-29 00:38:31:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:128
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:128) with 1 output partitions
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:128)
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128), which has no missing parents
2020-05-29 00:38:31:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 00:38:31:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-29 00:38:31:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.106:55512 (size: 1793.0 B, free: 2004.6 MB)
2020-05-29 00:38:31:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:128) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:38:31:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 00:38:31:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:38:31:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 43 ms on localhost (executor driver) (1/1)
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:128) finished in 0.044 s
2020-05-29 00:38:31:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:128, took 0.062307 s
2020-05-29 00:38:32:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.106:55512 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-29 00:38:32:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.106:55512 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 00:38:32:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 00:38:32:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 00:38:32:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5c234920{/SQL,null,AVAILABLE,@Spark}
2020-05-29 00:38:32:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@38087342{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:32:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5a4464c5{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 00:38:32:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7ab63838{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 00:38:32:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a056b26{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 00:38:33:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 00:38:34:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 199.287204 ms
2020-05-29 00:38:34:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-29 00:38:34:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-29 00:38:34:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-29 00:38:34:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.106:55512 (size: 6.5 KB, free: 2004.6 MB)
2020-05-29 00:38:34:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 00:38:34:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 00:38:34:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 00:38:34:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 20.348433 ms
2020-05-29 00:38:34:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
scala.MatchError: [Ljava.lang.String;@195ef666 (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87) ~[classes/:?]
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87) ~[classes/:?]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.immutable.List.foreach(List.scala:392) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.immutable.List.map(List.scala:296) ~[scala-library-2.11.11.jar:?]
	at core.XmlTransformer$.temp(XmlTransformer.scala:87) ~[classes/:?]
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:137) ~[classes/:?]
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:134) ~[classes/:?]
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2020-05-29 00:38:34:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@195ef666 (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at core.XmlTransformer$.temp(XmlTransformer.scala:87)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:137)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:134)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2020-05-29 00:38:34:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) failed in 0.125 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@195ef666 (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at core.XmlTransformer$.temp(XmlTransformer.scala:87)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:137)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:134)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
2020-05-29 00:38:34:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:57, took 0.149644 s
2020-05-29 00:38:34:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Invoking stop() from shutdown hook
2020-05-29 00:38:34:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStop() : Line.310} - Stopped Spark@138eb6dc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 15:02:52:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 15:02:53:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 15:02:53:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 15:02:54:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 15:02:54:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 15:02:54:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 15:02:54:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 15:02:54:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 15:02:54:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 59607.
2020-05-29 15:02:54:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 15:02:54:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 15:02:54:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 15:02:54:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 15:02:54:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-4ed57912-4dc7-4174-968d-4befa9d8e4c1
2020-05-29 15:02:54:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 15:02:54:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @8074ms
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @8329ms
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@2596792f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 15:02:55:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@204abeff{/jobs,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3679d92e{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58fa5769{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@ba17be6{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@332bcab0{/stages,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6342ff7f{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2daf06fc{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31e130bf{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@f1f7db2{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7c3e4b1a{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@765d55d5{/storage,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2bfb583b{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6fc1020a{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2629d5dc{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42a0501e{/environment,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6e4599c0{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3d1f558a{/executors,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28f4f300{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6ca8fcf3{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@66933239{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41ad373{/static,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7be1ce6a{/,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6dc9da2d{/api,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@79518e00{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7d70638{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 15:02:55:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 15:02:55:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 15:02:55:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59608.
2020-05-29 15:02:55:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:59608
2020-05-29 15:02:55:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 15:02:55:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 59608, None)
2020-05-29 15:02:55:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:59608 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 59608, None)
2020-05-29 15:02:55:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 59608, None)
2020-05-29 15:02:55:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 59608, None)
2020-05-29 15:02:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@500051c5{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:48:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 15:07:48:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 15:07:49:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 15:07:49:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 15:07:49:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 15:07:49:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 15:07:49:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 15:07:49:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 15:07:50:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 59663.
2020-05-29 15:07:50:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 15:07:50:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 15:07:50:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 15:07:50:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 15:07:50:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-617498cf-2890-4d1b-9b27-6d0dabb0d837
2020-05-29 15:07:50:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 15:07:50:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @6381ms
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @6541ms
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@1b27dbf2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 15:07:50:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6c0905f6{/jobs,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3456558{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6cbe7d4d{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58fa5769{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ee25d80{/stages,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@ba17be6{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@332bcab0{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5ceecfee{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28ee7bee{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31e130bf{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@f1f7db2{/storage,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7c3e4b1a{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@765d55d5{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2bfb583b{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6fc1020a{/environment,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2629d5dc{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42a0501e{/executors,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6e4599c0{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3d1f558a{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28f4f300{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6ca8fcf3{/static,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5740ff5e{/,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@67f77f6e{/api,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@68a78f3c{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3481ff98{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 15:07:50:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 15:07:51:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 15:07:51:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59664.
2020-05-29 15:07:51:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:59664
2020-05-29 15:07:51:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 15:07:51:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 59664, None)
2020-05-29 15:07:51:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:59664 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 59664, None)
2020-05-29 15:07:51:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 59664, None)
2020-05-29 15:07:51:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 59664, None)
2020-05-29 15:07:51:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6e829e50{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 15:32:12:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 15:32:12:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 15:32:12:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 15:32:12:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 15:32:12:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 15:32:12:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 15:32:12:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 15:32:12:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 60044.
2020-05-29 15:32:13:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 15:32:13:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 15:32:13:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 15:32:13:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 15:32:13:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-6df5bd06-3ad5-4b91-8311-0e51e5b39980
2020-05-29 15:32:13:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 15:32:13:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @4785ms
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @4899ms
2020-05-29 15:32:13:WARN WARN : org.apache.spark.util.Utils {logWarning() : Line.66} - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@2be95d31{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-05-29 15:32:13:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4041.
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1391af3b{/jobs,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@16a35bd{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6f798482{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@403c3a01{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28237492{/stages,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7da31a40{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1b5a1d85{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2db4ad1{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2513a118{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@73ae0257{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5762658b{/storage,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2596d7f4{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6aa3bfc{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7dffda8b{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6abdec0e{/environment,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2b5c4f17{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69f0b0f4{/executors,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2f7efd0b{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6801b414{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4f327096{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78a515e4{/static,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@17ba57f0{/,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2ddb3ae8{/api,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30c4e352{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@73809e7{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 15:32:13:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4041
2020-05-29 15:32:13:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 15:32:13:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60045.
2020-05-29 15:32:13:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:60045
2020-05-29 15:32:13:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 15:32:13:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 60045, None)
2020-05-29 15:32:13:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:60045 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 60045, None)
2020-05-29 15:32:13:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 60045, None)
2020-05-29 15:32:13:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 60045, None)
2020-05-29 15:32:14:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@56b8aaf1{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 15:32:14:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStop() : Line.310} - Stopped Spark@2be95d31{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-05-29 15:32:14:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Stopped Spark web UI at http://192.168.0.107:4041
2020-05-29 15:32:14:INFO INFO : org.apache.spark.MapOutputTrackerMasterEndpoint {logInfo() : Line.54} - MapOutputTrackerMasterEndpoint stopped!
2020-05-29 15:32:14:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore cleared
2020-05-29 15:32:14:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - BlockManager stopped
2020-05-29 15:32:14:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - BlockManagerMaster stopped
2020-05-29 15:32:14:INFO INFO : org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint {logInfo() : Line.54} - OutputCommitCoordinator stopped!
2020-05-29 15:32:14:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Successfully stopped SparkContext
2020-05-29 15:37:56:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 15:37:56:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 15:37:57:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 15:37:57:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 15:37:57:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 15:37:57:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 15:37:57:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 15:37:57:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 15:37:58:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 60185.
2020-05-29 15:37:58:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 15:37:58:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 15:37:58:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 15:37:58:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 15:37:58:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-a765f22a-8435-459d-a0de-840c8622f5de
2020-05-29 15:37:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 15:37:58:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 15:37:58:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @6531ms
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @6795ms
2020-05-29 15:37:59:WARN WARN : org.apache.spark.util.Utils {logWarning() : Line.66} - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@7d2e6634{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-05-29 15:37:59:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4041.
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5a034157{/jobs,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@36aa52d2{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@16a35bd{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@633cc6b5{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@403c3a01{/stages,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28237492{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7da31a40{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4462efe1{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2db4ad1{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2513a118{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@73ae0257{/storage,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5762658b{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2596d7f4{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6aa3bfc{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7dffda8b{/environment,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6abdec0e{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2b5c4f17{/executors,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69f0b0f4{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2f7efd0b{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6801b414{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4f327096{/static,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d408060{/,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@17ba57f0{/api,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@732f6050{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30c4e352{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 15:37:59:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4041
2020-05-29 15:37:59:ERROR ERROR: org.apache.spark.SparkContext {logError() : Line.91} - Error initializing SparkContext.
org.apache.spark.SparkException: Invalid master URL: spark://192.168.0.107:4041/
	at org.apache.spark.util.Utils$.extractHostPortFromSparkUrl(Utils.scala:2409) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rpc.RpcAddress$.fromSparkURL(RpcAddress.scala:47) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.deploy.client.StandaloneAppClient$$anonfun$1.apply(StandaloneAppClient.scala:52) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.deploy.client.StandaloneAppClient$$anonfun$1.apply(StandaloneAppClient.scala:52) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) ~[scala-library-2.11.11.jar:?]
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.deploy.client.StandaloneAppClient.<init>(StandaloneAppClient.scala:52) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.start(StandaloneSchedulerBackend.scala:115) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:509) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at core.Main$.main(Main.scala:54) [classes/:?]
	at core.Main.main(Main.scala) [classes/:?]
2020-05-29 15:37:59:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStop() : Line.310} - Stopped Spark@7d2e6634{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-05-29 15:37:59:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Stopped Spark web UI at http://192.168.0.107:4041
2020-05-29 15:37:59:INFO INFO : org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend {logInfo() : Line.54} - Shutting down all executors
2020-05-29 15:37:59:INFO INFO : org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint {logInfo() : Line.54} - Asking each executor to shut down
2020-05-29 15:38:00:ERROR ERROR: org.apache.spark.util.Utils {logError() : Line.91} - Uncaught exception in thread main
java.lang.NullPointerException: null
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:220) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:123) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:517) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1670) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1927) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:587) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at core.Main$.main(Main.scala:54) [classes/:?]
	at core.Main.main(Main.scala) [classes/:?]
2020-05-29 15:38:00:INFO INFO : org.apache.spark.MapOutputTrackerMasterEndpoint {logInfo() : Line.54} - MapOutputTrackerMasterEndpoint stopped!
2020-05-29 15:38:00:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore cleared
2020-05-29 15:38:00:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - BlockManager stopped
2020-05-29 15:38:00:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - BlockManagerMaster stopped
2020-05-29 15:38:00:WARN WARN : org.apache.spark.metrics.MetricsSystem {logWarning() : Line.66} - Stopping a MetricsSystem that is not running
2020-05-29 15:38:00:INFO INFO : org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint {logInfo() : Line.54} - OutputCommitCoordinator stopped!
2020-05-29 15:38:00:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Successfully stopped SparkContext
2020-05-29 15:41:23:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 15:41:24:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 15:41:24:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 15:41:24:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 15:41:24:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 15:41:24:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 15:41:24:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 15:41:24:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 15:41:25:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 60277.
2020-05-29 15:41:25:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 15:41:25:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 15:41:25:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 15:41:25:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 15:41:25:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-1e9b7305-99bd-4374-9b95-4c0dee334dcd
2020-05-29 15:41:25:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 15:41:25:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @4507ms
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @4630ms
2020-05-29 15:41:25:WARN WARN : org.apache.spark.util.Utils {logWarning() : Line.66} - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@4664c6f7{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-05-29 15:41:25:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4041.
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a45d714{/jobs,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6cbe7d4d{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3679d92e{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ee25d80{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@ba17be6{/stages,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@332bcab0{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6342ff7f{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28ee7bee{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31e130bf{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@f1f7db2{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7c3e4b1a{/storage,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@765d55d5{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2bfb583b{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6fc1020a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2629d5dc{/environment,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42a0501e{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6e4599c0{/executors,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3d1f558a{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28f4f300{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6ca8fcf3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@66933239{/static,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@67f77f6e{/,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7be1ce6a{/api,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3481ff98{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@79518e00{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 15:41:25:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4041
2020-05-29 15:41:25:ERROR ERROR: org.apache.spark.SparkContext {logError() : Line.91} - Error initializing SparkContext.
org.apache.spark.SparkException: Invalid master URL: spark://192.168.0.107:4041/
	at org.apache.spark.util.Utils$.extractHostPortFromSparkUrl(Utils.scala:2409) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rpc.RpcAddress$.fromSparkURL(RpcAddress.scala:47) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.deploy.client.StandaloneAppClient$$anonfun$1.apply(StandaloneAppClient.scala:52) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.deploy.client.StandaloneAppClient$$anonfun$1.apply(StandaloneAppClient.scala:52) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) ~[scala-library-2.11.11.jar:?]
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.deploy.client.StandaloneAppClient.<init>(StandaloneAppClient.scala:52) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.start(StandaloneSchedulerBackend.scala:115) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:509) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at core.Main$.main(Main.scala:54) [classes/:?]
	at core.Main.main(Main.scala) [classes/:?]
2020-05-29 15:41:25:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStop() : Line.310} - Stopped Spark@4664c6f7{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-05-29 15:41:25:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Stopped Spark web UI at http://192.168.0.107:4041
2020-05-29 15:41:25:INFO INFO : org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend {logInfo() : Line.54} - Shutting down all executors
2020-05-29 15:41:26:INFO INFO : org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint {logInfo() : Line.54} - Asking each executor to shut down
2020-05-29 15:41:26:ERROR ERROR: org.apache.spark.util.Utils {logError() : Line.91} - Uncaught exception in thread main
java.lang.NullPointerException: null
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:220) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:123) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:517) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1670) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1927) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:587) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516) [spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at scala.Option.getOrElse(Option.scala:121) [scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910) [spark-sql_2.11-2.2.1.jar:2.2.1]
	at core.Main$.main(Main.scala:54) [classes/:?]
	at core.Main.main(Main.scala) [classes/:?]
2020-05-29 15:41:26:INFO INFO : org.apache.spark.MapOutputTrackerMasterEndpoint {logInfo() : Line.54} - MapOutputTrackerMasterEndpoint stopped!
2020-05-29 15:41:26:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore cleared
2020-05-29 15:41:26:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - BlockManager stopped
2020-05-29 15:41:26:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - BlockManagerMaster stopped
2020-05-29 15:41:26:WARN WARN : org.apache.spark.metrics.MetricsSystem {logWarning() : Line.66} - Stopping a MetricsSystem that is not running
2020-05-29 15:41:26:INFO INFO : org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint {logInfo() : Line.54} - OutputCommitCoordinator stopped!
2020-05-29 15:41:26:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Successfully stopped SparkContext
2020-05-29 15:41:26:INFO INFO : org.apache.spark.util.ShutdownHookManager {logInfo() : Line.54} - Shutdown hook called
2020-05-29 15:42:08:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 15:42:08:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 15:42:09:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 15:42:09:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 15:42:09:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 15:42:09:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 15:42:09:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 15:42:09:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 15:42:09:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 60290.
2020-05-29 15:42:09:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 15:42:09:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 15:42:09:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 15:42:09:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 15:42:09:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-50b4f2f6-f648-4225-afe8-f80e2c28d5d5
2020-05-29 15:42:09:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 15:42:09:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @4546ms
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @4708ms
2020-05-29 15:42:10:WARN WARN : org.apache.spark.util.Utils {logWarning() : Line.66} - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@49f05756{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-05-29 15:42:10:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4041.
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@204abeff{/jobs,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3679d92e{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58fa5769{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@ba17be6{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@332bcab0{/stages,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6342ff7f{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2daf06fc{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31e130bf{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@f1f7db2{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7c3e4b1a{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@765d55d5{/storage,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2bfb583b{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6fc1020a{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2629d5dc{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42a0501e{/environment,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6e4599c0{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3d1f558a{/executors,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28f4f300{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6ca8fcf3{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@66933239{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41ad373{/static,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7be1ce6a{/,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6dc9da2d{/api,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@79518e00{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7d70638{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 15:42:10:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4041
2020-05-29 15:42:10:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 15:42:10:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60291.
2020-05-29 15:42:10:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:60291
2020-05-29 15:42:10:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 15:42:10:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 60291, None)
2020-05-29 15:42:10:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:60291 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 60291, None)
2020-05-29 15:42:10:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 60291, None)
2020-05-29 15:42:10:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 60291, None)
2020-05-29 15:42:11:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1095d23a{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 15:42:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at Main.scala:60
2020-05-29 15:42:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at Main.scala:60) with 1 output partitions
2020-05-29 15:42:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at Main.scala:60)
2020-05-29 15:42:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 15:42:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 15:42:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:59), which has no missing parents
2020-05-29 15:42:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 1288.0 B, free 2004.6 MB)
2020-05-29 15:42:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 920.0 B, free 2004.6 MB)
2020-05-29 15:42:11:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:60291 (size: 920.0 B, free: 2004.6 MB)
2020-05-29 15:42:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2020-05-29 15:42:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:59) (first 15 tasks are for partitions Vector(0))
2020-05-29 15:42:11:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5049 bytes)
2020-05-29 15:42:12:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 15:42:12:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 110 ms on localhost (executor driver) (1/1)
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at Main.scala:60) finished in 0.143 s
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at Main.scala:60, took 0.443855 s
2020-05-29 15:42:12:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at Main.scala:65
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (foreach at Main.scala:65) with 1 output partitions
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (foreach at Main.scala:65)
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (ParallelCollectionRDD[1] at parallelize at Main.scala:64), which has no missing parents
2020-05-29 15:42:12:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 1288.0 B, free 2004.6 MB)
2020-05-29 15:42:12:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 917.0 B, free 2004.6 MB)
2020-05-29 15:42:12:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:60291 (size: 917.0 B, free: 2004.6 MB)
2020-05-29 15:42:12:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[1] at parallelize at Main.scala:64) (first 15 tasks are for partitions Vector(0))
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5044 bytes)
2020-05-29 15:42:12:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 15:42:12:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 12 ms on localhost (executor driver) (1/1)
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (foreach at Main.scala:65) finished in 0.013 s
2020-05-29 15:42:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: foreach at Main.scala:65, took 0.035959 s
2020-05-29 15:46:19:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at Main.scala:68
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (foreach at Main.scala:68) with 1 output partitions
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (foreach at Main.scala:68)
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (ParallelCollectionRDD[2] at parallelize at Main.scala:67), which has no missing parents
2020-05-29 15:46:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 1288.0 B, free 2004.6 MB)
2020-05-29 15:46:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 918.0 B, free 2004.6 MB)
2020-05-29 15:46:19:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.107:60291 (size: 918.0 B, free: 2004.6 MB)
2020-05-29 15:46:19:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[2] at parallelize at Main.scala:67) (first 15 tasks are for partitions Vector(0))
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 5044 bytes)
2020-05-29 15:46:19:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 15:46:19:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2) in 17 ms on localhost (executor driver) (1/1)
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (foreach at Main.scala:68) finished in 0.022 s
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 finished: foreach at Main.scala:68, took 0.052728 s
2020-05-29 15:46:19:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStop() : Line.310} - Stopped Spark@49f05756{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-05-29 15:46:19:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Stopped Spark web UI at http://192.168.0.107:4041
2020-05-29 15:46:19:INFO INFO : org.apache.spark.MapOutputTrackerMasterEndpoint {logInfo() : Line.54} - MapOutputTrackerMasterEndpoint stopped!
2020-05-29 15:46:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore cleared
2020-05-29 15:46:19:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - BlockManager stopped
2020-05-29 15:46:19:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - BlockManagerMaster stopped
2020-05-29 15:46:19:INFO INFO : org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint {logInfo() : Line.54} - OutputCommitCoordinator stopped!
2020-05-29 15:46:19:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Successfully stopped SparkContext
2020-05-29 15:48:45:INFO INFO : org.apache.spark.util.ShutdownHookManager {logInfo() : Line.54} - Shutdown hook called
2020-05-29 15:48:55:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 15:48:55:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 15:48:55:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 15:48:55:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 15:48:55:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 15:48:55:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 15:48:55:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 15:48:55:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 15:48:56:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 60524.
2020-05-29 15:48:56:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 15:48:56:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 15:48:56:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 15:48:56:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 15:48:56:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-d313825e-430d-42bd-b567-b998a526fad1
2020-05-29 15:48:56:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 15:48:56:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 15:48:56:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @5893ms
2020-05-29 15:48:56:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 15:48:56:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @6020ms
2020-05-29 15:48:56:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@7455e43b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 15:48:56:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6c0905f6{/jobs,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3456558{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6cbe7d4d{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58fa5769{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ee25d80{/stages,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@ba17be6{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@332bcab0{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5ceecfee{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28ee7bee{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31e130bf{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@f1f7db2{/storage,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7c3e4b1a{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@765d55d5{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2bfb583b{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6fc1020a{/environment,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2629d5dc{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42a0501e{/executors,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6e4599c0{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3d1f558a{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@28f4f300{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6ca8fcf3{/static,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5740ff5e{/,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@67f77f6e{/api,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@68a78f3c{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3481ff98{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 15:48:57:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 15:48:57:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 15:48:57:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60525.
2020-05-29 15:48:57:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:60525
2020-05-29 15:48:57:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 15:48:57:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 60525, None)
2020-05-29 15:48:57:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:60525 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 60525, None)
2020-05-29 15:48:57:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 60525, None)
2020-05-29 15:48:57:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 60525, None)
2020-05-29 15:48:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6e829e50{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 15:48:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at Main.scala:60
2020-05-29 15:48:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at Main.scala:60) with 1 output partitions
2020-05-29 15:48:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at Main.scala:60)
2020-05-29 15:48:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 15:48:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 15:48:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:59), which has no missing parents
2020-05-29 15:48:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 1288.0 B, free 2004.6 MB)
2020-05-29 15:48:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 920.0 B, free 2004.6 MB)
2020-05-29 15:48:58:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:60525 (size: 920.0 B, free: 2004.6 MB)
2020-05-29 15:48:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2020-05-29 15:48:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:59) (first 15 tasks are for partitions Vector(0))
2020-05-29 15:48:58:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 15:48:58:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5049 bytes)
2020-05-29 15:48:58:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 15:48:58:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 132 ms on localhost (executor driver) (1/1)
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at Main.scala:60) finished in 0.178 s
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at Main.scala:60, took 0.502824 s
2020-05-29 15:48:59:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at Main.scala:65
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (foreach at Main.scala:65) with 1 output partitions
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (foreach at Main.scala:65)
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (ParallelCollectionRDD[1] at parallelize at Main.scala:64), which has no missing parents
2020-05-29 15:48:59:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 1288.0 B, free 2004.6 MB)
2020-05-29 15:48:59:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 917.0 B, free 2004.6 MB)
2020-05-29 15:48:59:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:60525 (size: 917.0 B, free: 2004.6 MB)
2020-05-29 15:48:59:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[1] at parallelize at Main.scala:64) (first 15 tasks are for partitions Vector(0))
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5044 bytes)
2020-05-29 15:48:59:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 15:48:59:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 10 ms on localhost (executor driver) (1/1)
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (foreach at Main.scala:65) finished in 0.012 s
2020-05-29 15:48:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: foreach at Main.scala:65, took 0.035923 s
2020-05-29 15:49:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at Main.scala:68
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (foreach at Main.scala:68) with 1 output partitions
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (foreach at Main.scala:68)
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (ParallelCollectionRDD[2] at parallelize at Main.scala:67), which has no missing parents
2020-05-29 15:49:17:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 1288.0 B, free 2004.6 MB)
2020-05-29 15:49:17:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 918.0 B, free 2004.6 MB)
2020-05-29 15:49:17:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.107:60525 (size: 918.0 B, free: 2004.6 MB)
2020-05-29 15:49:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[2] at parallelize at Main.scala:67) (first 15 tasks are for partitions Vector(0))
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 5044 bytes)
2020-05-29 15:49:17:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 15:49:17:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2) in 10 ms on localhost (executor driver) (1/1)
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (foreach at Main.scala:68) finished in 0.014 s
2020-05-29 15:49:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 finished: foreach at Main.scala:68, took 0.039099 s
2020-05-29 15:49:17:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStop() : Line.310} - Stopped Spark@7455e43b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 15:49:17:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Shutdown hook called
2020-05-29 16:07:31:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 16:07:32:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 16:07:32:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 16:07:32:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 16:07:32:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 16:07:32:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 16:07:32:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 16:07:32:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 16:07:33:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 60755.
2020-05-29 16:07:33:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 16:07:33:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 16:07:33:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 16:07:33:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 16:07:33:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-2a9d28d7-2619-4611-9cdf-bb7c317f3d59
2020-05-29 16:07:33:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 16:07:33:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 16:07:33:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @4974ms
2020-05-29 16:07:33:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 16:07:33:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @5093ms
2020-05-29 16:07:33:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@47a9cd1f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 16:07:33:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5aa6202e{/jobs,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2516fc68{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6bfdb014{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4aa3d36{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@347bdeef{/stages,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7f34a967{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d8e2eea{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@253c1256{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@503fbbc6{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@109f5dd8{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4a325eb9{/storage,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@57f64f5e{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@194152cf{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2c30b71f{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@ec50f54{/environment,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@22d6cac2{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1654a892{/executors,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3163987e{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5f233b26{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6974a715{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@43d455c9{/static,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@47dd778{/,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@36a7abe1{/api,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@16943e88{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@73d6d0c{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 16:07:34:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 16:07:34:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 16:07:34:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60758.
2020-05-29 16:07:34:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:60758
2020-05-29 16:07:34:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 16:07:34:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 60758, None)
2020-05-29 16:07:34:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:60758 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 60758, None)
2020-05-29 16:07:34:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 60758, None)
2020-05-29 16:07:34:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 60758, None)
2020-05-29 16:07:35:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@45792847{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 16:07:35:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at Main.scala:60
2020-05-29 16:07:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at Main.scala:60) with 1 output partitions
2020-05-29 16:07:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at Main.scala:60)
2020-05-29 16:07:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 16:07:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 16:07:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:59), which has no missing parents
2020-05-29 16:07:35:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 1288.0 B, free 2004.6 MB)
2020-05-29 16:07:35:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 920.0 B, free 2004.6 MB)
2020-05-29 16:07:35:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:60758 (size: 920.0 B, free: 2004.6 MB)
2020-05-29 16:07:35:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2020-05-29 16:07:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at Main.scala:59) (first 15 tasks are for partitions Vector(0))
2020-05-29 16:07:35:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 16:07:35:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5049 bytes)
2020-05-29 16:07:35:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 16:07:35:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 128 ms on localhost (executor driver) (1/1)
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at Main.scala:60) finished in 0.165 s
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at Main.scala:60, took 0.515964 s
2020-05-29 16:07:36:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at Main.scala:65
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (foreach at Main.scala:65) with 1 output partitions
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (foreach at Main.scala:65)
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (ParallelCollectionRDD[1] at parallelize at Main.scala:64), which has no missing parents
2020-05-29 16:07:36:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 1288.0 B, free 2004.6 MB)
2020-05-29 16:07:36:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 917.0 B, free 2004.6 MB)
2020-05-29 16:07:36:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:60758 (size: 917.0 B, free: 2004.6 MB)
2020-05-29 16:07:36:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[1] at parallelize at Main.scala:64) (first 15 tasks are for partitions Vector(0))
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5044 bytes)
2020-05-29 16:07:36:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 16:07:36:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 15 ms on localhost (executor driver) (1/1)
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (foreach at Main.scala:65) finished in 0.020 s
2020-05-29 16:07:36:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: foreach at Main.scala:65, took 0.051108 s
2020-05-29 16:07:42:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.107:60758 in memory (size: 917.0 B, free: 2004.6 MB)
2020-05-29 16:07:42:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_0_piece0 on 192.168.0.107:60758 in memory (size: 920.0 B, free: 2004.6 MB)
2020-05-29 23:30:13:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 23:30:13:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 23:30:14:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 23:30:14:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 23:30:14:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 23:30:14:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 23:30:14:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 23:30:14:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 23:30:14:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 49849.
2020-05-29 23:30:15:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 23:30:15:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 23:30:15:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 23:30:15:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 23:30:15:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-dbc48812-4b3f-483b-bd21-638eeaf438f9
2020-05-29 23:30:15:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 23:30:15:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @6510ms
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @6712ms
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@4d45b8b0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 23:30:15:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@65cc8228{/jobs,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11a7ba62{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30404dba{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@231baf51{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@73877e19{/stages,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5bfc257{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7cedfa63{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e8e8621{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3af356f{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7e46d648{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2b0b4d53{/storage,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@38548b19{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@303a5119{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1bcb79c2{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6fca2a8f{/environment,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@71391b3f{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@12db3386{/executors,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@79a1728c{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41f35f7c{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3005db4a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@198ef2ce{/static,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@417d6615{/,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@21325036{/api,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@48c4245d{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7906578e{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 23:30:15:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 23:30:15:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 23:30:15:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49850.
2020-05-29 23:30:15:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:49850
2020-05-29 23:30:15:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 23:30:15:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 49850, None)
2020-05-29 23:30:15:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:49850 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 49850, None)
2020-05-29 23:30:15:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 49850, None)
2020-05-29 23:30:15:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 49850, None)
2020-05-29 23:30:16:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@62628e78{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:16:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 23:30:16:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 23:30:16:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:49850 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 23:30:16:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 23:30:17:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:30:17:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:30:17:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 23:30:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:129
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:129) with 1 output partitions
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:129)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 23:30:17:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:49850 (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:30:17:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 880 bytes result sent to driver
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 279 ms on localhost (executor driver) (1/1)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:129) finished in 0.308 s
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:129, took 0.458565 s
2020-05-29 23:30:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:130
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:130) with 1 output partitions
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:130)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:130), which has no missing parents
2020-05-29 23:30:17:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1788.0 B, free 2004.4 MB)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.107:49850 (size: 1788.0 B, free: 2004.6 MB)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:130) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:30:17:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 33 ms on localhost (executor driver) (1/1)
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:130) finished in 0.034 s
2020-05-29 23:30:17:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:130, took 0.050027 s
2020-05-29 23:30:18:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.107:49850 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:30:18:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.107:49850 in memory (size: 1788.0 B, free: 2004.6 MB)
2020-05-29 23:30:19:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 23:30:19:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 23:30:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d7f8467{/SQL,null,AVAILABLE,@Spark}
2020-05-29 23:30:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7caf1e5{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2eb1c615{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 23:30:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d9d2119{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 23:30:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@61b838f2{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 23:30:19:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 23:30:20:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 209.794135 ms
2020-05-29 23:30:20:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:57
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:57) with 1 output partitions
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:57)
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57), which has no missing parents
2020-05-29 23:30:20:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-29 23:30:20:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-29 23:30:20:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.107:49850 (size: 6.5 KB, free: 2004.6 MB)
2020-05-29 23:30:20:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:57) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:30:20:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 23:30:20:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:30:20:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 16.518624 ms
2020-05-29 23:30:20:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 60.78369 ms
2020-05-29 23:30:20:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2). 1247 bytes result sent to driver
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2) in 161 ms on localhost (executor driver) (1/1)
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:57) finished in 0.162 s
2020-05-29 23:30:20:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 finished: show at XmlTransformerTest.scala:57, took 0.187529 s
2020-05-29 23:31:06:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 23:31:06:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 23:31:07:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 23:31:07:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 23:31:07:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 23:31:07:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 23:31:07:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 23:31:07:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 23:31:08:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 49865.
2020-05-29 23:31:08:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 23:31:08:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 23:31:08:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 23:31:08:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 23:31:08:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-e6347783-5601-4e91-9654-c3beebd21e10
2020-05-29 23:31:08:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 23:31:08:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7480ms
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @7776ms
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@c94d4cc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 23:31:09:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23da79eb{/jobs,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6723610b{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@60df7989{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@47f04e4d{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@388d14e{/stages,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@59939293{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d74c81b{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78d71df1{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a9c5b75{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dac121d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e28fee1{/storage,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3b332962{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2e7bf7b7{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de81be1{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4519f676{/environment,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3596b249{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@781711b7{/executors,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642ee49c{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69909c14{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e224df5{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5f5827d0{/static,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@414f87a9{/,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3cd26422{/api,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2ed71727{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@65bb6275{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 23:31:09:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 23:31:10:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 23:31:10:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49867.
2020-05-29 23:31:10:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:49867
2020-05-29 23:31:10:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 23:31:10:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 49867, None)
2020-05-29 23:31:10:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:49867 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 49867, None)
2020-05-29 23:31:10:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 49867, None)
2020-05-29 23:31:10:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 49867, None)
2020-05-29 23:31:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5e0c4f21{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 23:31:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 23:31:11:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:49867 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 23:31:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 23:31:11:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:31:11:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:31:11:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 23:31:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:129
2020-05-29 23:31:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:129) with 1 output partitions
2020-05-29 23:31:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:129)
2020-05-29 23:31:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:31:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:31:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 23:31:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 23:31:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-29 23:31:11:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:49867 (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:31:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:31:12:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 23:31:12:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:31:12:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 276 ms on localhost (executor driver) (1/1)
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:129) finished in 0.320 s
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:129, took 0.540868 s
2020-05-29 23:31:12:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:130
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:130) with 1 output partitions
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:130)
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:130), which has no missing parents
2020-05-29 23:31:12:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 23:31:12:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1788.0 B, free 2004.4 MB)
2020-05-29 23:31:12:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.107:49867 (size: 1788.0 B, free: 2004.6 MB)
2020-05-29 23:31:12:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:130) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:31:12:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 23:31:12:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:31:12:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 916 bytes result sent to driver
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 93 ms on localhost (executor driver) (1/1)
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:130) finished in 0.095 s
2020-05-29 23:31:12:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:130, took 0.111129 s
2020-05-29 23:31:17:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.107:49867 in memory (size: 1788.0 B, free: 2004.6 MB)
2020-05-29 23:31:17:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.107:49867 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:31:19:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 23:31:19:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 23:31:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@757c685d{/SQL,null,AVAILABLE,@Spark}
2020-05-29 23:31:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1ce57bbb{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@362c48c{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 23:31:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1983a4e4{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 23:31:19:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2b540131{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 23:31:20:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 23:31:21:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 331.896969 ms
2020-05-29 23:31:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:58
2020-05-29 23:31:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:58) with 1 output partitions
2020-05-29 23:31:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:58)
2020-05-29 23:31:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:31:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:31:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58), which has no missing parents
2020-05-29 23:31:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-29 23:31:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-29 23:31:21:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.107:49867 (size: 6.5 KB, free: 2004.6 MB)
2020-05-29 23:31:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:31:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:31:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 23:31:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:31:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 23:31:21:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:31:21:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 26.301805 ms
2020-05-29 23:50:03:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 23:50:03:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 23:50:04:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 23:50:04:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 23:50:04:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 23:50:04:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 23:50:04:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 23:50:04:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 23:50:05:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 50090.
2020-05-29 23:50:05:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 23:50:05:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 23:50:05:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 23:50:05:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 23:50:05:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-ba7b9e85-87d2-43a1-896a-d212fdde94cb
2020-05-29 23:50:06:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 23:50:06:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 23:50:06:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7333ms
2020-05-29 23:50:06:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 23:50:06:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @7816ms
2020-05-29 23:50:06:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@c6f6210{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 23:50:06:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@9f674ac{/jobs,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5cf8676a{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3520963d{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@68b366e2{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@10b687f2{/stages,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@26837057{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@991cbde{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@568750b7{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@16a2ed51{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@57ddd45b{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2fb25f4c{/storage,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@342e690b{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78ec89a6{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7237f3c1{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642a16aa{/environment,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@294aba23{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d3b58ca{/executors,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@32456db0{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58a2d9f9{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d02af26{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dd90166{/static,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@416d56f2{/,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1b01a0d{/api,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@47c7a9e5{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ed9f7b1{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 23:50:07:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 23:50:07:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 23:50:07:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50091.
2020-05-29 23:50:07:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:50091
2020-05-29 23:50:07:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 23:50:07:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 50091, None)
2020-05-29 23:50:07:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:50091 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 50091, None)
2020-05-29 23:50:07:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 50091, None)
2020-05-29 23:50:07:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 50091, None)
2020-05-29 23:50:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@19dc0d32{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:09:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 23:50:09:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 23:50:09:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:50091 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 23:50:09:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 23:50:10:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:50:10:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:50:10:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 23:50:10:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:130
2020-05-29 23:50:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:130) with 1 output partitions
2020-05-29 23:50:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:130)
2020-05-29 23:50:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:50:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:50:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 23:50:10:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 23:50:10:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1709.0 B, free 2004.4 MB)
2020-05-29 23:50:10:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:50091 (size: 1709.0 B, free: 2004.6 MB)
2020-05-29 23:50:10:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:50:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:50:10:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 23:50:10:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:50:10:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 23:50:10:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:50:11:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 350 ms on localhost (executor driver) (1/1)
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:130) finished in 0.385 s
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:130, took 0.698390 s
2020-05-29 23:50:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:131
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:131) with 1 output partitions
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:131)
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:131), which has no missing parents
2020-05-29 23:50:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 23:50:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1785.0 B, free 2004.4 MB)
2020-05-29 23:50:11:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.107:50091 (size: 1785.0 B, free: 2004.6 MB)
2020-05-29 23:50:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:131) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:50:11:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 23:50:11:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:50:11:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 109 ms on localhost (executor driver) (1/1)
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:131) finished in 0.111 s
2020-05-29 23:50:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:131, took 0.127736 s
2020-05-29 23:50:14:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.107:50091 in memory (size: 1709.0 B, free: 2004.6 MB)
2020-05-29 23:50:14:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.107:50091 in memory (size: 1785.0 B, free: 2004.6 MB)
2020-05-29 23:50:17:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 23:50:17:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 23:50:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6b576ff8{/SQL,null,AVAILABLE,@Spark}
2020-05-29 23:50:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@21022cbb{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@212c0aff{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 23:50:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@47666b39{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 23:50:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5ac7550a{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 23:50:17:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 23:50:19:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 287.167068 ms
2020-05-29 23:50:19:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:58
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:58) with 1 output partitions
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:58)
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58), which has no missing parents
2020-05-29 23:50:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-29 23:50:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-29 23:50:19:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.107:50091 (size: 6.5 KB, free: 2004.6 MB)
2020-05-29 23:50:19:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:50:19:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 23:50:19:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:50:19:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 23.723507 ms
2020-05-29 23:50:19:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
scala.MatchError: [Ljava.lang.String;@8e7b701 (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87) ~[classes/:?]
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87) ~[classes/:?]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.immutable.List.foreach(List.scala:392) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.immutable.List.map(List.scala:296) ~[scala-library-2.11.11.jar:?]
	at core.XmlTransformer$.temp(XmlTransformer.scala:87) ~[classes/:?]
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:140) ~[classes/:?]
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:137) ~[classes/:?]
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2020-05-29 23:50:19:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@8e7b701 (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at core.XmlTransformer$.temp(XmlTransformer.scala:87)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:140)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:137)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2020-05-29 23:50:19:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:58) failed in 0.336 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@8e7b701 (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:87)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at core.XmlTransformer$.temp(XmlTransformer.scala:87)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:140)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:137)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
2020-05-29 23:50:19:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:58, took 0.372251 s
2020-05-29 23:50:19:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Invoking stop() from shutdown hook
2020-05-29 23:52:19:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 23:52:19:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 23:52:20:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 23:52:20:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 23:52:20:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 23:52:20:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 23:52:20:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 23:52:20:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 23:52:21:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 50131.
2020-05-29 23:52:21:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 23:52:21:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 23:52:21:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 23:52:21:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 23:52:21:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-715b9fb3-4d00-4d96-ab5e-9990180ceb0d
2020-05-29 23:52:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 23:52:21:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7156ms
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @7322ms
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@4dfe8126{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 23:52:21:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75023c53{/jobs,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@16a2ed51{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@57ddd45b{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@342e690b{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78ec89a6{/stages,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7237f3c1{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642a16aa{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@32456db0{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58a2d9f9{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d02af26{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dd90166{/storage,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5ad1904f{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@f438904{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@18d003cd{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@22ad1bae{/environment,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@59c04bee{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@f2a1813{/executors,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@22bdb1d0{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@388623ad{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@46e3559f{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@32118208{/static,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@580fd26b{/,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1290ed28{/api,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3610f277{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41fa769c{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 23:52:21:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 23:52:22:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 23:52:22:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50132.
2020-05-29 23:52:22:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:50132
2020-05-29 23:52:22:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 23:52:22:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 50132, None)
2020-05-29 23:52:22:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:50132 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 50132, None)
2020-05-29 23:52:22:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 50132, None)
2020-05-29 23:52:22:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 50132, None)
2020-05-29 23:52:22:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@184823ed{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 23:52:23:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 23:52:23:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 23:52:23:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:50132 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 23:52:23:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 23:52:23:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:52:23:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:52:23:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 23:52:23:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:131
2020-05-29 23:52:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:131) with 1 output partitions
2020-05-29 23:52:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:131)
2020-05-29 23:52:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:52:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:52:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 23:52:23:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 23:52:23:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-29 23:52:23:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:50132 (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:52:23:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:52:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:52:23:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:52:24:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 23:52:24:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:52:24:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 321 ms on localhost (executor driver) (1/1)
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:131) finished in 0.359 s
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:131, took 0.682296 s
2020-05-29 23:52:24:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:132
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:132) with 1 output partitions
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:132)
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:132), which has no missing parents
2020-05-29 23:52:24:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 23:52:24:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1792.0 B, free 2004.4 MB)
2020-05-29 23:52:24:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.107:50132 (size: 1792.0 B, free: 2004.6 MB)
2020-05-29 23:52:24:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:132) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:52:24:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 23:52:24:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:52:24:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 80 ms on localhost (executor driver) (1/1)
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:132) finished in 0.082 s
2020-05-29 23:52:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:132, took 0.109267 s
2020-05-29 23:53:04:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.107:50132 in memory (size: 1792.0 B, free: 2004.6 MB)
2020-05-29 23:53:04:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.107:50132 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:53:06:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 23:53:06:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 23:53:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3d3a28b5{/SQL,null,AVAILABLE,@Spark}
2020-05-29 23:53:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@38a52072{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 23:53:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2dbb8da0{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 23:53:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@17fb5184{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 23:53:06:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@526fc044{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 23:53:06:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 23:53:07:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 282.893334 ms
2020-05-29 23:53:07:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:58
2020-05-29 23:53:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:58) with 1 output partitions
2020-05-29 23:53:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:58)
2020-05-29 23:53:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:53:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:53:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58), which has no missing parents
2020-05-29 23:53:07:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-29 23:53:07:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-29 23:53:07:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.107:50132 (size: 6.5 KB, free: 2004.6 MB)
2020-05-29 23:53:07:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:53:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:53:07:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 23:53:07:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:53:07:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 23:53:07:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:53:07:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 22.188181 ms
2020-05-29 23:54:52:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 23:54:53:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 23:54:54:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 23:54:54:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 23:54:54:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 23:54:54:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 23:54:54:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 23:54:54:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 23:54:55:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 50165.
2020-05-29 23:54:55:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 23:54:55:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 23:54:55:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 23:54:55:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 23:54:55:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-b6972c4a-2fe1-4338-b654-009e20dc499d
2020-05-29 23:54:55:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 23:54:55:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 23:54:55:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @8814ms
2020-05-29 23:54:55:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @9080ms
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@4dfe8126{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 23:54:56:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23da79eb{/jobs,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6723610b{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@60df7989{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@47f04e4d{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@388d14e{/stages,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@59939293{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d74c81b{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78d71df1{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a9c5b75{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dac121d{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e28fee1{/storage,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3b332962{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2e7bf7b7{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de81be1{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4519f676{/environment,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3596b249{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@781711b7{/executors,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642ee49c{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69909c14{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e224df5{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5f5827d0{/static,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@414f87a9{/,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3cd26422{/api,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2ed71727{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@65bb6275{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 23:54:56:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 23:54:56:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 23:54:56:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50166.
2020-05-29 23:54:56:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:50166
2020-05-29 23:54:56:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 23:54:56:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 50166, None)
2020-05-29 23:54:56:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:50166 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 50166, None)
2020-05-29 23:54:56:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 50166, None)
2020-05-29 23:54:56:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 50166, None)
2020-05-29 23:54:57:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1df5c7e3{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 23:54:57:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 23:54:57:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 23:54:57:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:50166 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 23:54:57:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 23:54:58:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:54:58:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:54:58:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 23:54:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:131
2020-05-29 23:54:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:131) with 1 output partitions
2020-05-29 23:54:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:131)
2020-05-29 23:54:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:54:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:54:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 23:54:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 23:54:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-29 23:54:58:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:50166 (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:54:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:54:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:54:58:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 23:54:58:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:54:58:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 23:54:58:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:54:59:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 383 ms on localhost (executor driver) (1/1)
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:131) finished in 0.420 s
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:131, took 0.745712 s
2020-05-29 23:54:59:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:132
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:132) with 1 output partitions
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:132)
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:132), which has no missing parents
2020-05-29 23:54:59:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 23:54:59:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1792.0 B, free 2004.4 MB)
2020-05-29 23:54:59:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.107:50166 (size: 1792.0 B, free: 2004.6 MB)
2020-05-29 23:54:59:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:132) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:54:59:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 23:54:59:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:54:59:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 60 ms on localhost (executor driver) (1/1)
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:132) finished in 0.063 s
2020-05-29 23:54:59:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:132, took 0.082955 s
2020-05-29 23:55:02:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.107:50166 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:55:02:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.107:50166 in memory (size: 1792.0 B, free: 2004.6 MB)
2020-05-29 23:55:03:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 23:55:03:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 23:55:03:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@cd93621{/SQL,null,AVAILABLE,@Spark}
2020-05-29 23:55:03:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@21ba0d33{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 23:55:03:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d98364c{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 23:55:03:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1fd35a92{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 23:55:03:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2435c6ae{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 23:55:04:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 23:55:05:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 336.219487 ms
2020-05-29 23:55:05:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:58
2020-05-29 23:55:05:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:58) with 1 output partitions
2020-05-29 23:55:05:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:58)
2020-05-29 23:55:05:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:55:05:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:55:05:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58), which has no missing parents
2020-05-29 23:55:05:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-29 23:55:05:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-29 23:55:05:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.107:50166 (size: 6.5 KB, free: 2004.6 MB)
2020-05-29 23:55:05:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:55:05:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:55:05:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 23:55:05:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:55:05:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 23:55:05:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:55:05:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 22.090407 ms
2020-05-29 23:56:35:ERROR ERROR: org.apache.spark.executor.Executor {logError() : Line.91} - Exception in task 0.0 in stage 2.0 (TID 2)
scala.MatchError: [Ljava.lang.String;@60109152 (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:88) ~[classes/:?]
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:88) ~[classes/:?]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.immutable.List.foreach(List.scala:392) ~[scala-library-2.11.11.jar:?]
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[scala-library-2.11.11.jar:?]
	at scala.collection.immutable.List.map(List.scala:296) ~[scala-library-2.11.11.jar:?]
	at core.XmlTransformer$.temp(XmlTransformer.scala:88) ~[classes/:?]
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:141) ~[classes/:?]
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:138) ~[classes/:?]
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) ~[scala-library-2.11.11.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) ~[scala-library-2.11.11.jar:?]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228) ~[spark-sql_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:108) ~[spark-core_2.11-2.2.1.jar:2.2.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) [spark-core_2.11-2.2.1.jar:2.2.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2020-05-29 23:56:35:WARN WARN : org.apache.spark.scheduler.TaskSetManager {logWarning() : Line.66} - Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@60109152 (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:88)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:88)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at core.XmlTransformer$.temp(XmlTransformer.scala:88)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:141)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:138)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2020-05-29 23:56:35:ERROR ERROR: org.apache.spark.scheduler.TaskSetManager {logError() : Line.70} - Task 0 in stage 2.0 failed 1 times; aborting job
2020-05-29 23:56:35:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-29 23:56:35:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Cancelling stage 2
2020-05-29 23:56:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:58) failed in 89.727 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@60109152 (of class [Ljava.lang.String;)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:88)
	at core.XmlTransformer$$anonfun$2.apply(XmlTransformer.scala:88)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:296)
	at core.XmlTransformer$.temp(XmlTransformer.scala:88)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:141)
	at core.XmlTransformer$$anonfun$4.apply(XmlTransformer.scala:138)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
2020-05-29 23:56:35:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 failed: show at XmlTransformerTest.scala:58, took 89.763207 s
2020-05-29 23:57:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 23:57:59:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 23:57:59:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 23:57:59:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 23:57:59:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 23:57:59:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 23:57:59:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 23:57:59:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 23:58:00:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 50199.
2020-05-29 23:58:00:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 23:58:00:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 23:58:00:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 23:58:00:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 23:58:00:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-a8c0fcb6-dceb-411a-855d-9557cd357c48
2020-05-29 23:58:00:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 23:58:00:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 23:58:00:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @5744ms
2020-05-29 23:58:00:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @5904ms
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@67bd6af7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 23:58:01:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@45545e7a{/jobs,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6e60f18{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@47f04e4d{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@59939293{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d74c81b{/stages,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@55f4887d{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2dbc408c{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dac121d{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e28fee1{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3b332962{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2e7bf7b7{/storage,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de81be1{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4519f676{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3596b249{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@781711b7{/environment,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642ee49c{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69909c14{/executors,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e224df5{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5f5827d0{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4337afd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3fa7df1{/static,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3fa21d49{/,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6e31d989{/api,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@72a2312e{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7951c3a2{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 23:58:01:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 23:58:01:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 23:58:01:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50200.
2020-05-29 23:58:01:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:50200
2020-05-29 23:58:01:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 23:58:01:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 50200, None)
2020-05-29 23:58:01:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:50200 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 50200, None)
2020-05-29 23:58:01:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 50200, None)
2020-05-29 23:58:01:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 50200, None)
2020-05-29 23:58:01:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@25953be6{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:02:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 23:58:03:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 23:58:03:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:50200 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 23:58:03:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 23:58:03:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:58:03:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:58:03:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 23:58:03:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:135
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:135) with 1 output partitions
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:135)
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 23:58:03:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 23:58:03:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-29 23:58:03:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:50200 (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:58:03:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:58:03:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 23:58:03:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:58:03:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 305 ms on localhost (executor driver) (1/1)
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:135) finished in 0.345 s
2020-05-29 23:58:03:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:135, took 0.568599 s
2020-05-29 23:58:04:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:136
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:136) with 1 output partitions
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:136)
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:136), which has no missing parents
2020-05-29 23:58:04:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 23:58:04:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1792.0 B, free 2004.4 MB)
2020-05-29 23:58:04:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.107:50200 (size: 1792.0 B, free: 2004.6 MB)
2020-05-29 23:58:04:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:136) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:58:04:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 23:58:04:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:58:04:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 54 ms on localhost (executor driver) (1/1)
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:136) finished in 0.056 s
2020-05-29 23:58:04:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:136, took 0.074040 s
2020-05-29 23:58:10:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.107:50200 in memory (size: 1792.0 B, free: 2004.6 MB)
2020-05-29 23:58:10:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.107:50200 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:58:12:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 23:58:12:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 23:58:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@397dfbe8{/SQL,null,AVAILABLE,@Spark}
2020-05-29 23:58:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3791af{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@178c6261{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 23:58:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2555a986{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@e3692ca{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 23:58:12:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 23:58:13:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 367.073619 ms
2020-05-29 23:58:14:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:58
2020-05-29 23:58:14:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:58) with 1 output partitions
2020-05-29 23:58:14:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:58)
2020-05-29 23:58:14:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:58:14:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:58:14:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58), which has no missing parents
2020-05-29 23:58:14:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-29 23:58:14:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-29 23:58:14:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.107:50200 (size: 6.5 KB, free: 2004.6 MB)
2020-05-29 23:58:14:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:58:14:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:58:14:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 23:58:14:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:58:14:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 23:58:14:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:58:14:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 20.490851 ms
2020-05-29 23:58:50:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-29 23:58:51:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-29 23:58:52:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-29 23:58:52:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-29 23:58:52:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-29 23:58:52:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-29 23:58:52:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-29 23:58:52:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-29 23:58:53:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 50212.
2020-05-29 23:58:53:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-29 23:58:53:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-29 23:58:53:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-29 23:58:53:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-29 23:58:53:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-ae964b3d-c203-41e2-8f6c-4f401b122548
2020-05-29 23:58:53:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-29 23:58:53:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-29 23:58:53:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @9644ms
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @9921ms
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@c6f6210{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-29 23:58:54:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@da28d03{/jobs,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@991cbde{/jobs/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@456bcb74{/jobs/job,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@568750b7{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@16a2ed51{/stages,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@57ddd45b{/stages/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2fb25f4c{/stages/stage,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7237f3c1{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642a16aa{/stages/pool,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@294aba23{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d3b58ca{/storage,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@32456db0{/storage/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@58a2d9f9{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1d02af26{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dd90166{/environment,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5ad1904f{/environment/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@f438904{/executors,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@18d003cd{/executors/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@22ad1bae{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@59c04bee{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@f2a1813{/static,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@57e388c3{/,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@21bd128b{/api,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@580fd26b{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1290ed28{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-29 23:58:54:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-29 23:58:54:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-29 23:58:55:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50213.
2020-05-29 23:58:55:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:50213
2020-05-29 23:58:55:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-29 23:58:55:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 50213, None)
2020-05-29 23:58:55:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:50213 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 50213, None)
2020-05-29 23:58:55:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 50213, None)
2020-05-29 23:58:55:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 50213, None)
2020-05-29 23:58:55:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@25953be6{/metrics/json,null,AVAILABLE,@Spark}
2020-05-29 23:58:56:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-29 23:58:56:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-29 23:58:56:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:50213 (size: 20.5 KB, free: 2004.6 MB)
2020-05-29 23:58:56:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-29 23:58:57:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:58:57:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-29 23:58:57:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-29 23:58:57:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:135
2020-05-29 23:58:57:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:135) with 1 output partitions
2020-05-29 23:58:57:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:135)
2020-05-29 23:58:57:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:58:57:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:58:57:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-29 23:58:57:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-29 23:58:57:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-29 23:58:57:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:50213 (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:58:57:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:58:57:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:58:57:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-29 23:58:57:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:58:57:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-29 23:58:57:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:58:58:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 401 ms on localhost (executor driver) (1/1)
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:135) finished in 0.443 s
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:135, took 0.714828 s
2020-05-29 23:58:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:136
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:136) with 1 output partitions
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:136)
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:136), which has no missing parents
2020-05-29 23:58:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-29 23:58:58:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1792.0 B, free 2004.4 MB)
2020-05-29 23:58:58:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.107:50213 (size: 1792.0 B, free: 2004.6 MB)
2020-05-29 23:58:58:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:136) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:58:58:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-29 23:58:58:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:58:58:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 916 bytes result sent to driver
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 51 ms on localhost (executor driver) (1/1)
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:136) finished in 0.053 s
2020-05-29 23:58:58:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:136, took 0.071719 s
2020-05-29 23:59:08:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.107:50213 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-29 23:59:08:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.107:50213 in memory (size: 1792.0 B, free: 2004.6 MB)
2020-05-29 23:59:09:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-29 23:59:09:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-29 23:59:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5e9f1a4c{/SQL,null,AVAILABLE,@Spark}
2020-05-29 23:59:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@457b8fc3{/SQL/json,null,AVAILABLE,@Spark}
2020-05-29 23:59:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3da61af2{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-29 23:59:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@417751d3{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-29 23:59:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3d3a28b5{/static/sql,null,AVAILABLE,@Spark}
2020-05-29 23:59:10:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-29 23:59:11:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 514.647469 ms
2020-05-29 23:59:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:58
2020-05-29 23:59:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:58) with 1 output partitions
2020-05-29 23:59:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:58)
2020-05-29 23:59:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-29 23:59:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-29 23:59:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58), which has no missing parents
2020-05-29 23:59:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-29 23:59:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-29 23:59:11:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.107:50213 (size: 6.5 KB, free: 2004.6 MB)
2020-05-29 23:59:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-29 23:59:11:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58) (first 15 tasks are for partitions Vector(0))
2020-05-29 23:59:11:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-29 23:59:11:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-29 23:59:11:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-29 23:59:11:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-29 23:59:11:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 20.455293 ms
2020-05-30 00:09:02:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-30 00:09:02:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-30 00:09:03:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-30 00:09:03:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-30 00:09:03:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-30 00:09:03:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-30 00:09:03:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-30 00:09:03:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-30 00:09:04:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 50321.
2020-05-30 00:09:04:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-30 00:09:04:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-30 00:09:04:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-30 00:09:04:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-30 00:09:04:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-e4f3e8ba-0822-4ce7-b7f4-5c923a3622f8
2020-05-30 00:09:04:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-30 00:09:04:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @5773ms
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @6009ms
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@46f30a81{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-30 00:09:04:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a799159{/jobs,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30b2b76f{/jobs/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23ee75c5{/jobs/job,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@30404dba{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c0884e8{/stages,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11841b15{/stages/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/stages/stage,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e83c18{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/pool,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c446b14{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/storage,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/storage/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/environment,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/environment/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/executors,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/executors/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/static,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4bee18dc{/,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44c5a16f{/api,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6de30571{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3c89bb12{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-30 00:09:04:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.107:4040
2020-05-30 00:09:05:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-30 00:09:05:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50322.
2020-05-30 00:09:05:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.107:50322
2020-05-30 00:09:05:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-30 00:09:05:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.107, 50322, None)
2020-05-30 00:09:05:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.107:50322 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.107, 50322, None)
2020-05-30 00:09:05:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.107, 50322, None)
2020-05-30 00:09:05:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.107, 50322, None)
2020-05-30 00:09:05:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6f89292e{/metrics/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:06:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-30 00:09:06:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-30 00:09:06:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.107:50322 (size: 20.5 KB, free: 2004.6 MB)
2020-05-30 00:09:06:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-30 00:09:06:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-30 00:09:06:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-30 00:09:06:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-30 00:09:06:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: foreach at XmlTransformer.scala:132
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (foreach at XmlTransformer.scala:132) with 1 output partitions
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (foreach at XmlTransformer.scala:132)
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37), which has no missing parents
2020-05-30 00:09:06:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 2004.4 MB)
2020-05-30 00:09:06:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1711.0 B, free 2004.4 MB)
2020-05-30 00:09:06:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.107:50322 (size: 1711.0 B, free: 2004.6 MB)
2020-05-30 00:09:06:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at XmlTransformerTest.scala:37) (first 15 tasks are for partitions Vector(0))
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-30 00:09:06:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-30 00:09:06:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-30 00:09:06:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 320 ms on localhost (executor driver) (1/1)
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (foreach at XmlTransformer.scala:132) finished in 0.364 s
2020-05-30 00:09:06:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: foreach at XmlTransformer.scala:132, took 0.527646 s
2020-05-30 00:09:07:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformer.scala:133
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformer.scala:133) with 1 output partitions
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformer.scala:133)
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:133), which has no missing parents
2020-05-30 00:09:07:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 3.0 KB, free 2004.4 MB)
2020-05-30 00:09:07:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1793.0 B, free 2004.4 MB)
2020-05-30 00:09:07:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.107:50322 (size: 1793.0 B, free: 2004.6 MB)
2020-05-30 00:09:07:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at XmlTransformer.scala:133) (first 15 tasks are for partitions Vector(0))
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-30 00:09:07:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-30 00:09:07:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-30 00:09:07:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 873 bytes result sent to driver
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 47 ms on localhost (executor driver) (1/1)
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformer.scala:133) finished in 0.049 s
2020-05-30 00:09:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformer.scala:133, took 0.063347 s
2020-05-30 00:09:08:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_2_piece0 on 192.168.0.107:50322 in memory (size: 1793.0 B, free: 2004.6 MB)
2020-05-30 00:09:08:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.107:50322 in memory (size: 1711.0 B, free: 2004.6 MB)
2020-05-30 00:09:08:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-30 00:09:08:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-30 00:09:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e17442f{/SQL,null,AVAILABLE,@Spark}
2020-05-30 00:09:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@62e73ab6{/SQL/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7ddeb27f{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-30 00:09:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@681de87f{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-30 00:09:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7ab63838{/static/sql,null,AVAILABLE,@Spark}
2020-05-30 00:09:09:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-30 00:09:10:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 231.208011 ms
2020-05-30 00:09:10:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:58
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 2 (show at XmlTransformerTest.scala:58) with 1 output partitions
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 2 (show at XmlTransformerTest.scala:58)
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58), which has no missing parents
2020-05-30 00:09:10:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-30 00:09:10:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-30 00:09:10:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_3_piece0 in memory on 192.168.0.107:50322 (size: 6.5 KB, free: 2004.6 MB)
2020-05-30 00:09:10:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at show at XmlTransformerTest.scala:58) (first 15 tasks are for partitions Vector(0))
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 2.0 with 1 tasks
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-30 00:09:10:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 2.0 (TID 2)
2020-05-30 00:09:10:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-30 00:09:10:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 23.981827 ms
2020-05-30 00:09:10:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 93.386161 ms
2020-05-30 00:09:10:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2). 1205 bytes result sent to driver
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 2.0 (TID 2) in 213 ms on localhost (executor driver) (1/1)
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 2 (show at XmlTransformerTest.scala:58) finished in 0.214 s
2020-05-30 00:09:10:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 2 finished: show at XmlTransformerTest.scala:58, took 0.239695 s
2020-05-30 23:05:05:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-30 23:05:05:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-30 23:05:06:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-30 23:05:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-30 23:05:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-30 23:05:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-30 23:05:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-30 23:05:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-30 23:05:07:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 52002.
2020-05-30 23:05:07:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-30 23:05:07:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-30 23:05:07:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-30 23:05:07:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-30 23:05:08:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-f27df02f-bc63-4943-b53d-c00499d0fb69
2020-05-30 23:05:08:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-30 23:05:08:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-30 23:05:08:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @7336ms
2020-05-30 23:05:08:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @7631ms
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@7240b2b0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-30 23:05:09:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6ff37443{/jobs,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23ee75c5{/jobs/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@340b7ef6{/jobs/job,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c0884e8{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11841b15{/stages,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/stages/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@34523d46{/stages/stage,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c446b14{/stages/pool,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/storage,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/storage/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/environment,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/environment/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/executors,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/executors/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@425d5d46{/static,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44c5a16f{/,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a6ebe1e{/api,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3c89bb12{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3df978b9{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-30 23:05:09:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-30 23:05:09:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-30 23:05:09:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52003.
2020-05-30 23:05:09:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:52003
2020-05-30 23:05:09:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-30 23:05:09:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 52003, None)
2020-05-30 23:05:09:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:52003 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 52003, None)
2020-05-30 23:05:09:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 52003, None)
2020-05-30 23:05:09:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 52003, None)
2020-05-30 23:05:10:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de77232{/metrics/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-30 23:05:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-30 23:05:11:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:52003 (size: 20.5 KB, free: 2004.6 MB)
2020-05-30 23:05:11:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-30 23:05:13:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-30 23:05:13:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-30 23:05:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3360283{/SQL,null,AVAILABLE,@Spark}
2020-05-30 23:05:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@73e4bb60{/SQL/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1304e0d7{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-30 23:05:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7767bd4e{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-30 23:05:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1068176{/static/sql,null,AVAILABLE,@Spark}
2020-05-30 23:05:14:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-30 23:05:15:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 360.990244 ms
2020-05-30 23:05:15:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-30 23:05:15:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-30 23:05:15:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-30 23:05:15:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:58
2020-05-30 23:05:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (show at XmlTransformerTest.scala:58) with 1 output partitions
2020-05-30 23:05:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (show at XmlTransformerTest.scala:58)
2020-05-30 23:05:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-30 23:05:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-30 23:05:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[6] at show at XmlTransformerTest.scala:58), which has no missing parents
2020-05-30 23:05:15:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-30 23:05:15:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-30 23:05:15:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:52003 (size: 6.5 KB, free: 2004.6 MB)
2020-05-30 23:05:15:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-30 23:05:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at show at XmlTransformerTest.scala:58) (first 15 tasks are for partitions Vector(0))
2020-05-30 23:05:15:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-30 23:05:15:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-30 23:05:15:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-30 23:05:15:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-30 23:05:15:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 25.834715 ms
2020-05-30 23:05:16:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 82.116344 ms
2020-05-30 23:05:16:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 1205 bytes result sent to driver
2020-05-30 23:05:16:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 442 ms on localhost (executor driver) (1/1)
2020-05-30 23:05:16:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-30 23:05:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (show at XmlTransformerTest.scala:58) finished in 0.470 s
2020-05-30 23:05:16:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: show at XmlTransformerTest.scala:58, took 0.646589 s
2020-05-30 23:10:04:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-30 23:10:05:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-30 23:10:06:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-30 23:10:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-30 23:10:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-30 23:10:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-30 23:10:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-30 23:10:06:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-30 23:10:06:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 52070.
2020-05-30 23:10:07:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-30 23:10:07:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-30 23:10:07:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-30 23:10:07:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-30 23:10:07:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-c4427129-2268-4dd2-afb1-0e6dac825ad5
2020-05-30 23:10:07:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-30 23:10:07:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-30 23:10:07:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @9781ms
2020-05-30 23:10:07:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-30 23:10:07:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @9982ms
2020-05-30 23:10:07:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@b88381d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-30 23:10:07:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@20ab76ee{/jobs,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d74c81b{/jobs/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@55f4887d{/jobs/job,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@78d71df1{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a9c5b75{/stages,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4dac121d{/stages/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e28fee1{/stages/stage,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de81be1{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4519f676{/stages/pool,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3596b249{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@781711b7{/storage,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@642ee49c{/storage/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@69909c14{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4e224df5{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5f5827d0{/environment,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4337afd{/environment/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3fa7df1{/executors,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@52227eb2{/executors/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a146b11{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4ed5a1b0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3135bf25{/static,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@72a2312e{/,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7951c3a2{/api,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d0b05{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2b916808{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-30 23:10:08:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-30 23:10:08:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-30 23:10:08:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52072.
2020-05-30 23:10:08:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:52072
2020-05-30 23:10:08:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-30 23:10:08:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 52072, None)
2020-05-30 23:10:08:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:52072 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 52072, None)
2020-05-30 23:10:08:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 52072, None)
2020-05-30 23:10:08:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 52072, None)
2020-05-30 23:10:09:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2bb62414{/metrics/json,null,AVAILABLE,@Spark}
2020-05-30 23:10:10:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-30 23:10:10:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-30 23:10:10:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:52072 (size: 20.5 KB, free: 2004.6 MB)
2020-05-30 23:10:10:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:36
2020-05-30 23:18:49:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-30 23:18:49:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-30 23:18:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@66130c3b{/SQL,null,AVAILABLE,@Spark}
2020-05-30 23:18:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42fa5cb{/SQL/json,null,AVAILABLE,@Spark}
2020-05-30 23:18:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@508b4f70{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-30 23:18:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7af56b26{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-30 23:18:49:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3d73cd78{/static/sql,null,AVAILABLE,@Spark}
2020-05-30 23:18:50:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-30 23:18:51:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 318.079327 ms
2020-05-30 23:18:51:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-30 23:18:51:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-30 23:18:51:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-30 23:18:51:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:58
2020-05-30 23:18:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (show at XmlTransformerTest.scala:58) with 1 output partitions
2020-05-30 23:18:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (show at XmlTransformerTest.scala:58)
2020-05-30 23:18:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-30 23:18:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-30 23:18:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[6] at show at XmlTransformerTest.scala:58), which has no missing parents
2020-05-30 23:18:52:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-30 23:18:52:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-30 23:18:52:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:52072 (size: 6.5 KB, free: 2004.6 MB)
2020-05-30 23:18:52:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-30 23:18:52:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at show at XmlTransformerTest.scala:58) (first 15 tasks are for partitions Vector(0))
2020-05-30 23:18:52:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-30 23:18:52:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-30 23:18:52:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-30 23:18:52:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-30 23:18:52:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 23.509412 ms
2020-05-30 23:20:07:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 112.0495 ms
2020-05-30 23:20:07:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 1248 bytes result sent to driver
2020-05-30 23:20:07:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 75409 ms on localhost (executor driver) (1/1)
2020-05-30 23:20:07:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-30 23:20:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (show at XmlTransformerTest.scala:58) finished in 75.438 s
2020-05-30 23:20:07:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: show at XmlTransformerTest.scala:58, took 75.634484 s
2020-05-30 23:20:14:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: collect at XmlTransformerTest.scala:59
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 1 (collect at XmlTransformerTest.scala:59) with 1 output partitions
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 1 (collect at XmlTransformerTest.scala:59)
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 1 (MapPartitionsRDD[3] at flatMap at XmlTransformer.scala:80), which has no missing parents
2020-05-30 23:20:15:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 2004.3 MB)
2020-05-30 23:20:15:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 2004.3 MB)
2020-05-30 23:20:15:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_2_piece0 in memory on 192.168.0.105:52072 (size: 2.4 KB, free: 2004.6 MB)
2020-05-30 23:20:15:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at flatMap at XmlTransformer.scala:80) (first 15 tasks are for partitions Vector(0))
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 1.0 with 1 tasks
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-30 23:20:15:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 1.0 (TID 1)
2020-05-30 23:20:15:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-30 23:20:15:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Removed broadcast_1_piece0 on 192.168.0.105:52072 in memory (size: 6.5 KB, free: 2004.6 MB)
2020-05-30 23:20:15:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1). 1092 bytes result sent to driver
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 1.0 (TID 1) in 61 ms on localhost (executor driver) (1/1)
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 1 (collect at XmlTransformerTest.scala:59) finished in 0.063 s
2020-05-30 23:20:15:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 1 finished: collect at XmlTransformerTest.scala:59, took 1.016092 s
2020-05-31 00:42:15:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-31 00:42:15:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-31 00:42:15:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-31 00:42:15:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-31 00:42:15:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-31 00:42:15:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-31 00:42:15:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-31 00:42:15:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-31 00:42:16:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 53373.
2020-05-31 00:42:16:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-31 00:42:16:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-31 00:42:16:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-31 00:42:16:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-31 00:42:16:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-c9229ea5-43cc-40bf-85d4-594df995455a
2020-05-31 00:42:16:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 366.3 MB
2020-05-31 00:42:16:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-31 00:42:16:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @33491ms
2020-05-31 00:42:16:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-31 00:42:16:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @33629ms
2020-05-31 00:42:16:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@31dc6554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-31 00:42:16:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@29348862{/jobs,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@8beac99{/jobs/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@540f46bf{/jobs/job,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4f568c82{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2b8411b0{/stages,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5a2d4c2f{/stages/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@20975a19{/stages/stage,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@9ce05b8{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5a87fac2{/stages/pool,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2e911615{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6584ea79{/storage,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4d525963{/storage/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@dff6f51{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@17747c33{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a69c324{/environment,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@13cf66d1{/environment/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7ec8dc17{/executors,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@417ffb6c{/executors/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51ff4442{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@311e4c1d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@b3ee5c8{/static,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7c1716c9{/,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5ec870db{/api,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1ed10b8{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3789a7e{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-31 00:42:17:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-31 00:42:17:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-31 00:42:18:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53374.
2020-05-31 00:42:18:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:53374
2020-05-31 00:42:18:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-31 00:42:18:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 53374, None)
2020-05-31 00:42:18:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:53374 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.105, 53374, None)
2020-05-31 00:42:18:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 53374, None)
2020-05-31 00:42:18:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 53374, None)
2020-05-31 00:42:18:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@208a335c{/metrics/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 366.1 MB)
2020-05-31 00:42:19:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 366.1 MB)
2020-05-31 00:42:19:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:53374 (size: 20.5 KB, free: 366.3 MB)
2020-05-31 00:42:19:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:26
2020-05-31 00:42:22:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-31 00:42:22:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-31 00:42:22:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@eee5001{/SQL,null,AVAILABLE,@Spark}
2020-05-31 00:42:22:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@73ed6372{/SQL/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:22:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1c4cbf85{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-31 00:42:22:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3e1d0170{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-31 00:42:22:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@e345a2e{/static/sql,null,AVAILABLE,@Spark}
2020-05-31 00:42:22:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-31 00:42:23:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 240.059726 ms
2020-05-31 00:42:23:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-31 00:42:23:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-31 00:42:23:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-31 00:42:23:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:48
2020-05-31 00:42:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (show at XmlTransformerTest.scala:48) with 1 output partitions
2020-05-31 00:42:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (show at XmlTransformerTest.scala:48)
2020-05-31 00:42:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-31 00:42:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-31 00:42:23:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[6] at show at XmlTransformerTest.scala:48), which has no missing parents
2020-05-31 00:42:24:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 13.1 KB, free 366.1 MB)
2020-05-31 00:42:24:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KB, free 366.1 MB)
2020-05-31 00:42:24:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:53374 (size: 6.5 KB, free: 366.3 MB)
2020-05-31 00:42:24:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-31 00:42:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at show at XmlTransformerTest.scala:48) (first 15 tasks are for partitions Vector(0))
2020-05-31 00:42:24:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-31 00:42:24:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-31 00:42:24:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-31 00:42:24:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-31 00:42:24:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 26.964308 ms
2020-05-31 00:42:24:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 104.987968 ms
2020-05-31 00:42:24:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 1248 bytes result sent to driver
2020-05-31 00:42:24:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 482 ms on localhost (executor driver) (1/1)
2020-05-31 00:42:24:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-31 00:42:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (show at XmlTransformerTest.scala:48) finished in 0.515 s
2020-05-31 00:42:24:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: show at XmlTransformerTest.scala:48, took 0.747722 s
2020-05-31 18:06:41:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-31 18:06:41:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-31 18:06:41:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-31 18:06:42:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-31 18:06:42:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-31 18:06:42:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-31 18:06:42:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-31 18:06:42:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-31 18:06:42:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 61976.
2020-05-31 18:06:42:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-31 18:06:42:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-31 18:06:42:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-31 18:06:42:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-31 18:06:42:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-2fec5c00-a9ce-476b-a529-e7202bcdbaaf
2020-05-31 18:06:42:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-31 18:06:43:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @5767ms
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @5948ms
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@569eef59{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-31 18:06:43:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6ff37443{/jobs,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23ee75c5{/jobs/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@340b7ef6{/jobs/job,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4c0884e8{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@11841b15{/stages,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7b208b45{/stages/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@34523d46{/stages/stage,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5d235104{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c446b14{/stages/pool,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4443ef6f{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@51751e5f{/storage,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7068f7ca{/storage/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@41aaedaa{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@75b3673{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@d1a10ac{/environment,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@31fc71ab{/environment/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2cfbeac4{/executors,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@4078695f{/executors/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@a7f0ab6{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@42c2f48c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@425d5d46{/static,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44c5a16f{/,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7a6ebe1e{/api,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3c89bb12{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3df978b9{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-31 18:06:43:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-31 18:06:43:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-31 18:06:43:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61977.
2020-05-31 18:06:43:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:61977
2020-05-31 18:06:43:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-31 18:06:43:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 61977, None)
2020-05-31 18:06:43:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:61977 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 61977, None)
2020-05-31 18:06:43:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 61977, None)
2020-05-31 18:06:43:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 61977, None)
2020-05-31 18:06:44:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@de77232{/metrics/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:44:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-31 18:06:44:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-31 18:06:44:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:61977 (size: 20.5 KB, free: 2004.6 MB)
2020-05-31 18:06:44:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at XmlTransformerTest.scala:26
2020-05-31 18:06:46:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-31 18:06:46:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-31 18:06:46:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3360283{/SQL,null,AVAILABLE,@Spark}
2020-05-31 18:06:46:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@73e4bb60{/SQL/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:46:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1304e0d7{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-31 18:06:46:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@7767bd4e{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-31 18:06:46:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1068176{/static/sql,null,AVAILABLE,@Spark}
2020-05-31 18:06:46:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-31 18:06:47:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 242.689243 ms
2020-05-31 18:06:47:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-31 18:06:47:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-31 18:06:47:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-31 18:06:48:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at XmlTransformerTest.scala:48
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (show at XmlTransformerTest.scala:48) with 1 output partitions
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (show at XmlTransformerTest.scala:48)
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[6] at show at XmlTransformerTest.scala:48), which has no missing parents
2020-05-31 18:06:48:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 13.1 KB, free 2004.4 MB)
2020-05-31 18:06:48:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-31 18:06:48:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:61977 (size: 6.5 KB, free: 2004.6 MB)
2020-05-31 18:06:48:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at show at XmlTransformerTest.scala:48) (first 15 tasks are for partitions Vector(0))
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-31 18:06:48:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-31 18:06:48:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-31 18:06:48:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 39.047151 ms
2020-05-31 18:06:48:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 78.9155 ms
2020-05-31 18:06:48:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 1204 bytes result sent to driver
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 402 ms on localhost (executor driver) (1/1)
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (show at XmlTransformerTest.scala:48) finished in 0.424 s
2020-05-31 18:06:48:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: show at XmlTransformerTest.scala:48, took 0.615930 s
2020-05-31 18:12:10:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Running Spark version 2.2.1
2020-05-31 18:12:10:WARN WARN : org.apache.hadoop.util.NativeCodeLoader {<clinit>() : Line.62} - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-05-31 18:12:10:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Submitted application: Name
2020-05-31 18:12:10:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls to: caca
2020-05-31 18:12:10:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls to: caca
2020-05-31 18:12:10:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing view acls groups to: 
2020-05-31 18:12:10:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - Changing modify acls groups to: 
2020-05-31 18:12:10:INFO INFO : org.apache.spark.SecurityManager {logInfo() : Line.54} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(caca); groups with view permissions: Set(); users  with modify permissions: Set(caca); groups with modify permissions: Set()
2020-05-31 18:12:11:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'sparkDriver' on port 62054.
2020-05-31 18:12:11:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering MapOutputTracker
2020-05-31 18:12:11:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering BlockManagerMaster
2020-05-31 18:12:11:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-05-31 18:12:11:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - BlockManagerMasterEndpoint up
2020-05-31 18:12:11:INFO INFO : org.apache.spark.storage.DiskBlockManager {logInfo() : Line.54} - Created local directory at /private/var/folders/d5/nwmlk15949578t0sqw6bhn5w0000gn/T/blockmgr-99d45f49-a22a-4953-a359-6caec75c1338
2020-05-31 18:12:11:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - MemoryStore started with capacity 2004.6 MB
2020-05-31 18:12:12:INFO INFO : org.apache.spark.SparkEnv {logInfo() : Line.54} - Registering OutputCommitCoordinator
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.util.log {initialized() : Line.192} - Logging initialized @5265ms
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.345} - jetty-9.3.z-SNAPSHOT
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.Server {doStart() : Line.403} - Started @5425ms
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.AbstractConnector {doStart() : Line.270} - Started ServerConnector@894858{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-05-31 18:12:12:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'SparkUI' on port 4040.
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@867ba60{/jobs,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@13cda7c9{/jobs/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3af9aa66{/jobs/job,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@62d0ac62{/jobs/job/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6826c41e{/stages,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@64d43929{/stages/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@437ebf59{/stages/stage,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1bdbf9be{/stages/stage/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@1da6ee17{/stages/pool,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3c818ac4{/stages/pool/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@71154f21{/storage,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2516fc68{/storage/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@6bfdb014{/storage/rdd,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@606fc505{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2d140a7{/environment,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2aa27288{/environment/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@77e80a5e{/executors,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@240139e1{/executors/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@49298ce7{/executors/threadDump,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@8dfe921{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@55f45b92{/static,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@44f9779c{/,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@5e8a459{/api,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@19542407{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@c7a977f{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-05-31 18:12:12:INFO INFO : org.apache.spark.ui.SparkUI {logInfo() : Line.54} - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.105:4040
2020-05-31 18:12:13:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Starting executor ID driver on host localhost
2020-05-31 18:12:13:INFO INFO : org.apache.spark.util.Utils {logInfo() : Line.54} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62056.
2020-05-31 18:12:13:INFO INFO : org.apache.spark.network.netty.NettyBlockTransferService {logInfo() : Line.54} - Server created on 192.168.0.105:62056
2020-05-31 18:12:13:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-05-31 18:12:13:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registering BlockManager BlockManagerId(driver, 192.168.0.105, 62056, None)
2020-05-31 18:12:13:INFO INFO : org.apache.spark.storage.BlockManagerMasterEndpoint {logInfo() : Line.54} - Registering block manager 192.168.0.105:62056 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.0.105, 62056, None)
2020-05-31 18:12:13:INFO INFO : org.apache.spark.storage.BlockManagerMaster {logInfo() : Line.54} - Registered BlockManager BlockManagerId(driver, 192.168.0.105, 62056, None)
2020-05-31 18:12:13:INFO INFO : org.apache.spark.storage.BlockManager {logInfo() : Line.54} - Initialized BlockManager: BlockManagerId(driver, 192.168.0.105, 62056, None)
2020-05-31 18:12:13:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@23e44287{/metrics/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:15:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0 stored as values in memory (estimated size 215.1 KB, free 2004.4 MB)
2020-05-31 18:12:15:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 2004.4 MB)
2020-05-31 18:12:15:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_0_piece0 in memory on 192.168.0.105:62056 (size: 20.5 KB, free: 2004.6 MB)
2020-05-31 18:12:15:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 0 from wholeTextFiles at Processor.scala:23
2020-05-31 18:12:18:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/').
2020-05-31 18:12:18:INFO INFO : org.apache.spark.sql.internal.SharedState {logInfo() : Line.54} - Warehouse path is 'file:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/spark-warehouse/'.
2020-05-31 18:12:18:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@16cb9989{/SQL,null,AVAILABLE,@Spark}
2020-05-31 18:12:18:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@3815146b{/SQL/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:18:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2dc39b53{/SQL/execution,null,AVAILABLE,@Spark}
2020-05-31 18:12:18:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@2f2e4bde{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-05-31 18:12:18:INFO INFO : org.spark_project.jetty.server.handler.ContextHandler {doStart() : Line.781} - Started o.s.j.s.ServletContextHandler@216372b7{/static/sql,null,AVAILABLE,@Spark}
2020-05-31 18:12:19:INFO INFO : org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef {logInfo() : Line.54} - Registered StateStoreCoordinator endpoint
2020-05-31 18:12:21:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 624.69278 ms
2020-05-31 18:12:21:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-31 18:12:21:INFO INFO : org.apache.hadoop.mapreduce.lib.input.FileInputFormat {listStatus() : Line.281} - Total input paths to process : 1
2020-05-31 18:12:21:INFO INFO : org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat {createSplits() : Line.413} - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2020-05-31 18:12:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Starting job: show at Processor.scala:40
2020-05-31 18:12:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Got job 0 (show at Processor.scala:40) with 1 output partitions
2020-05-31 18:12:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Final stage: ResultStage 0 (show at Processor.scala:40)
2020-05-31 18:12:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Parents of final stage: List()
2020-05-31 18:12:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Missing parents: List()
2020-05-31 18:12:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting ResultStage 0 (MapPartitionsRDD[6] at show at Processor.scala:40), which has no missing parents
2020-05-31 18:12:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1 stored as values in memory (estimated size 13.0 KB, free 2004.4 MB)
2020-05-31 18:12:21:INFO INFO : org.apache.spark.storage.memory.MemoryStore {logInfo() : Line.54} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KB, free 2004.4 MB)
2020-05-31 18:12:21:INFO INFO : org.apache.spark.storage.BlockManagerInfo {logInfo() : Line.54} - Added broadcast_1_piece0 in memory on 192.168.0.105:62056 (size: 6.5 KB, free: 2004.6 MB)
2020-05-31 18:12:21:INFO INFO : org.apache.spark.SparkContext {logInfo() : Line.54} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2020-05-31 18:12:21:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at show at Processor.scala:40) (first 15 tasks are for partitions Vector(0))
2020-05-31 18:12:21:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Adding task set 0.0 with 1 tasks
2020-05-31 18:12:21:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4971 bytes)
2020-05-31 18:12:21:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Running task 0.0 in stage 0.0 (TID 0)
2020-05-31 18:12:21:INFO INFO : org.apache.spark.rdd.WholeTextFileRDD {logInfo() : Line.54} - Input split: Paths:/Users/caca/Documents/code/temp/weird/XmlSparkProcessor/input/input.xml:0+393
2020-05-31 18:12:21:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 23.080183 ms
2020-05-31 18:12:22:INFO INFO : org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator {logInfo() : Line.54} - Code generated in 98.766187 ms
2020-05-31 18:12:22:INFO INFO : org.apache.spark.executor.Executor {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0). 1248 bytes result sent to driver
2020-05-31 18:12:22:INFO INFO : org.apache.spark.scheduler.TaskSetManager {logInfo() : Line.54} - Finished task 0.0 in stage 0.0 (TID 0) in 494 ms on localhost (executor driver) (1/1)
2020-05-31 18:12:22:INFO INFO : org.apache.spark.scheduler.TaskSchedulerImpl {logInfo() : Line.54} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2020-05-31 18:12:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - ResultStage 0 (show at Processor.scala:40) finished in 0.521 s
2020-05-31 18:12:22:INFO INFO : org.apache.spark.scheduler.DAGScheduler {logInfo() : Line.54} - Job 0 finished: show at Processor.scala:40, took 0.703962 s
